{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee60d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch # was 20230913, now 20231013\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d529c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.models.llama import LLaMAConfig, LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1504b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLaMA(\n",
       "  (shared): WordEmbedding(\n",
       "    (emb): Embedding(32000, 4096)\n",
       "    (head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LLaMABlock(\n",
       "      (ln): LayerNormParameterized()\n",
       "      (ff_ln): LayerNormParameterized()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (key): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (value): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (ff_sub_layer): GatedLinearUnit(\n",
       "        (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (wg): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (a): SiLU()\n",
       "        (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec_norm): LayerNormParameterized()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelc = LLaMAConfig(32000, 4096, 1e-6, 32, 0, 32)\n",
    "model = LLaMA(modelc)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4558b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(\"../../../llama_7b_ckp.pth\")['model_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce3e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = list(d.keys())\n",
    "for key in keylist:\n",
    "    if \"dec_process\" in key:\n",
    "        value = d.pop(key)\n",
    "        fields = key.split(\".\")\n",
    "        fields[0] = \"layers\"\n",
    "        d[\".\".join(fields)] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83097771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['rope.freqs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(d, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57035028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.utils.generation import generate\n",
    "from transformers import AutoTokenizer\n",
    "t = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "vinv = {v:k for k,v in t.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5616faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?']\n"
     ]
    }
   ],
   "source": [
    "inp = t(\"Hello, how are you today?\")[\"input_ids\"]\n",
    "print([vinv[x] for x in inp])\n",
    "inp = torch.IntTensor(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762260eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Hello, how are you today? I hope you are doing well. I am doing well. I am happy to be here with you'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle = generate(model, inp, 20, 20, do_sample=False, use_cache=True)\n",
    "t.decode(oracle.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6fd98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Speculator(n_heads=3)\n",
    "test.load_state_dict(torch.load(\"../../../specu_dist.pth\", map_location=\"cpu\")[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08810d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844107776"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in test.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aecabfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁I', '<0x0A>', '▁We', '▁My', '▁This']\n",
      "Topk@2: ['’', \"'\", '▁you', '▁hope', '▁is']\n",
      "Topk@3: ['▁you', '▁is', 'm', 's', '▁I']\n",
      "Verification: ['?', '▁I', '▁hope', '▁you'] ['▁I', '▁hope', '▁you', '▁are'] 3\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['re', '▁doing', '▁well', '▁having', '▁all']\n",
      "Topk@2: ['▁well', '▁doing', '▁having', '▁great', '.']\n",
      "Topk@3: ['▁well', '.', '▁great', '▁a', '▁good']\n",
      "Verification: ['▁are', '▁doing', '▁well', '.'] ['▁doing', '▁well', '.', '▁I'] 3\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁you', '’', ',', \"'\", '▁am']\n",
      "Topk@2: ['▁is', ',', 'm', '▁I', '▁you']\n",
      "Topk@3: ['▁I', ',', '▁a', '▁you', '▁to']\n",
      "Verification: ['▁I', '▁you', '▁is', '▁I'] ['▁am', '▁are', '▁a', '▁am'] 0\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁you', 'm', '▁to', '▁a', 've']\n",
      "Topk@2: ['▁to', '▁a', '▁been', '▁you', '▁excited']\n",
      "Topk@3: ['▁you', '▁to', '▁a', '▁today', '▁with']\n",
      "Verification: ['▁am', '▁you', '▁to', '▁you'] ['▁doing', '▁know', '▁share', '.'] 0\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁to', '▁a', '▁excited', '▁happy', '▁well']\n",
      "Topk@2: ['.', '▁to', '▁I', ',', '▁and']\n",
      "Topk@3: ['▁I', '▁you', '▁today', '▁and', '.']\n",
      "Verification: ['▁doing', '▁well', '.', '▁I'] ['▁well', '.', '▁I', '▁am'] 3\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁a', '▁been', '▁to', 'm', '▁you']\n",
      "Topk@2: ['▁to', '▁a', '▁been', 't', '▁happy']\n",
      "Topk@3: ['▁to', '▁a', '▁and', '▁the', '▁with']\n",
      "Verification: ['▁am', '▁a', '▁to', '▁to'] ['▁happy', '▁little', 'dd', '▁be'] 0\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁to', '▁happy', '▁a', '▁with', '▁excited']\n",
      "Topk@2: ['▁to', '.', '▁the', '▁with', '▁I']\n",
      "Topk@3: ['▁I', '.', '▁the', '▁you', '▁my']\n",
      "Verification: ['▁happy', '▁to', '▁to', '▁I'] ['▁to', '▁be', '▁be', '▁am'] 1\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁you', '▁that', '▁here', '▁this', '▁with']\n",
      "Topk@2: ['▁you', '▁that', '▁here', '▁today', '.']\n",
      "Topk@3: ['▁I', '.', '▁blog', '▁my', '▁you']\n",
      "Verification: ['▁be', '▁here', '▁you', '▁I'] ['▁here', '▁with', '▁know', '▁am'] 1\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁you', '.', '▁share', '▁I', '▁sharing']\n",
      "Topk@2: ['▁you', '.', '▁blog', '▁and', '▁I']\n",
      "Topk@3: ['▁you', '.', '▁I', '▁and', '▁to']\n",
      "Verification: ['▁with', '▁you', '.', '▁I'] ['▁you', '.', '▁I', '▁am'] 3\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with', '▁you', '.', '▁I', '▁am']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁you', '▁to', '▁glad', '▁happy', '▁a']\n",
      "Topk@2: ['▁to', '▁you', '▁happy', '▁thank', '▁excited']\n",
      "Topk@3: ['▁you', '▁to', '▁a', '▁and', '▁I']\n",
      "Verification: ['▁am', '▁happy', '▁to', '▁you'] ['▁happy', '▁to', '▁be', '.'] 2\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with', '▁you', '.', '▁I', '▁am', '▁happy', '▁to', '▁be']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁you', '▁here', '▁with', '▁able', '▁a']\n",
      "Topk@2: ['▁you', '.', '▁here', '▁to', '▁with']\n",
      "Topk@3: ['.', '▁you', '▁I', '▁of', '▁to']\n",
      "Verification: ['▁be', '▁here', '▁you', '.'] ['▁here', '▁with', '.', '▁I'] 1\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with', '▁you', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with']\n",
      "\n",
      "torch.Size([1, 1, 4096])\n",
      "Topk@1: ['▁you', '▁my', '▁your', '▁share', '▁this']\n",
      "Topk@2: ['.', '▁you', '▁blog', '▁family', '▁and']\n",
      "Topk@3: ['.', '▁you', '▁I', '▁and', '<0x0A>']\n",
      "Verification: ['▁with', '▁you', '.', '▁I'] ['▁you', '.', '▁I', '▁am'] 3\n",
      "Updated output: ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with', '▁you', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with', '▁you', '.', '▁I', '▁am']\n",
      "\n",
      "\n",
      "Steps: 12\n"
     ]
    }
   ],
   "source": [
    "out, steps = speculative_generate(model, inp, test, 30, 30, top_k=25)\n",
    "print()\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eeebbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.modules.layernorm import LayerNormParameterized\n",
    "\n",
    "\n",
    "class Speculator(nn.Module):\n",
    "    def __init__(self, emb_dim=4096, vocab_size=32000, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.nheads = n_heads\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vsize = vocab_size\n",
    "        self.w_in = nn.Parameter(torch.empty(emb_dim, int((emb_dim * 2.6875 * 2) // 256) * 256 * 2))  # d 2z\n",
    "        self.a = nn.GELU()\n",
    "        self.w_out = nn.Parameter(torch.empty(int((emb_dim * 2.6875 * 2) // 256) * 256, emb_dim * n_heads))  # z hd\n",
    "        self.ln = LayerNormParameterized(emb_dim, elementwise_shift=False, elementwise_scale=True)\n",
    "        self.head = nn.Parameter(torch.empty(n_heads, emb_dim, vocab_size))  # h d v\n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        nn.init.trunc_normal_(self.w_in, 0, (1 / 2.6875) ** (1 / 6) / self.emb_dim**0.5)\n",
    "        nn.init.trunc_normal_(self.w_out, 0, (1 / 2.6875) ** (1 / 6) / self.emb_dim**0.5)\n",
    "        nn.init.trunc_normal_(self.head, 0, 1 / self.emb_dim**0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: b n d\n",
    "        print(x.shape)\n",
    "        z, g = x.matmul(self.w_in).chunk(2, dim=2)\n",
    "        z = z * self.a(g)\n",
    "        z = z.matmul(self.w_out).view(x.size(0), x.size(1), self.nheads, self.emb_dim)  # b n h d\n",
    "        z = z + x.unsqueeze(2)\n",
    "        z = self.ln(z)\n",
    "        z = torch.einsum(\"bnhd,hdv->bnhv\", z, self.head)\n",
    "        return z # b n h v\n",
    "    \n",
    "    \n",
    "def get_topk_tree(logits, k=50):\n",
    "    # probs: b h v\n",
    "    n_adds = logits.size(1)\n",
    "    probs = logits.softmax(2)\n",
    "    # Generate probabilities for all combos of predictions\n",
    "    probtable = [\n",
    "        probs[:,i].view(\n",
    "            *([-1] + [1]*i + [probs.size(2)] + [1]*(n_adds-i-1))\n",
    "        ).expand(\n",
    "            *([-1] + [probs.size(2)]*n_adds)\n",
    "        )\n",
    "        for i in range(n_adds)\n",
    "    ]\n",
    "    probtable = torch.stack(probtable, 0).prod(0) # b v v v...\n",
    "    psize = probtable.size()\n",
    "    probtable = probtable.view(psize[0],-1)\n",
    "    # Fetch top-k most probable tree nodes\n",
    "    v,i = probtable.topk(k, dim=1) # b k\n",
    "    i = torch.stack(torch.unravel_index(i, psize[1:]), 1)\n",
    "    # v: b k\n",
    "    # i: b h k\n",
    "    return v,i\n",
    "\n",
    "    \n",
    "def decode_obo(x):\n",
    "    return [vinv[z] for z in x.squeeze().tolist()]\n",
    "\n",
    "\n",
    "def speculative_generate(\n",
    "    model: Union[Callable, torch.nn.Module],\n",
    "    input_ids: torch.LongTensor,\n",
    "    smallmodel: torch.nn.Module,\n",
    "    max_seq_len: int = 2048,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 25,\n",
    "    num_beams: int = 1,\n",
    "):\n",
    "    do_sample = False\n",
    "    batched = False\n",
    "    if num_beams != 1:\n",
    "        raise NotImplementedError(\"generate() does yet not support beam search\")\n",
    "    if type(input_ids) == torch.Tensor:\n",
    "        if input_ids.dim() != 1:\n",
    "            batched = True\n",
    "    else:\n",
    "        raise RuntimeError(\"generate() requires a tensor of token ids as the prefix\")\n",
    "\n",
    "    if not batched:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    result = input_ids\n",
    "    next_input = input_ids\n",
    "    kwargs = dict()\n",
    "    kwargs[\"past_key_value_states\"] = None\n",
    "    kwargs[\"use_cache\"] = True\n",
    "\n",
    "    output = model(input_ids[:,:-1], include_embeds=True, **kwargs)\n",
    "    _, past_key_value_states, embeds = output\n",
    "    kwargs[\"past_key_value_states\"] = past_key_value_states\n",
    "    next_input = next_input[:,-1:]\n",
    "    \n",
    "    n_gen = 0\n",
    "    n_steps = 0\n",
    "    n_kv_s = past_key_value_states\n",
    "    while n_gen < max_new_tokens:\n",
    "        n_steps += 1\n",
    "        input_ids = next_input[:, -max_seq_len:]\n",
    "        \n",
    "        probs = smallmodel(embeds[:,-1].unsqueeze(1)).squeeze(1) # b h v\n",
    "        probs, topk = probs.topk(5, dim=2) # b h 5\n",
    "        n_adds = smallmodel.nheads\n",
    "        for i in range(n_adds):\n",
    "            print(f\"Topk@{i+1}:\", decode_obo(topk[0,i]))\n",
    "        \n",
    "        # Build probability table\n",
    "        topk_v, topk_i = get_topk_tree(probs, top_k)\n",
    "        \n",
    "        # Assemble batch of tree branches\n",
    "        adds = topk.gather(2, topk_i).transpose(1,2) # b k h\n",
    "        adds = adds[0] # For now, non-batching and take only first b entry\n",
    "        input_ids = torch.cat([input_ids.expand(top_k,1), adds], dim=-1) \n",
    "#         print(\"Speculations:\")\n",
    "#         for i in range(top_k):\n",
    "#             print(decode_obo(input_ids[i]))\n",
    "        \n",
    "        mask = torch.ones(input_ids.size(1),input_ids.size(1)+n_kv_s[0][0].size(2))\n",
    "        mask = mask.tril(diagonal=mask.size(1)-mask.size(0))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0).log()\n",
    "        \n",
    "#         input_ids = input_ids[0].unsqueeze(0).expand(25,-1)\n",
    "        \n",
    "        output = model.forward(input_ids, include_embeds=True, mask=mask, **kwargs)\n",
    "        \n",
    "        logits, past_key_value_states, embeds = output\n",
    "        logits = logits[:, -n_adds-1:, :]\n",
    "\n",
    "        if do_sample:\n",
    "            # get logits from last value in sequence and scale\n",
    "#             logits = logits / temperature\n",
    "#             if top_k:\n",
    "#                 v, _ = torch.topk(logits, top_k)\n",
    "#                 logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "#             probs = F.softmax(logits, dim=-1)\n",
    "#             next_val = torch.multinomial(probs, num_samples=1)\n",
    "            assert False\n",
    "        else:\n",
    "            next_vals = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Check correctness of smallmodel predictions\n",
    "        test = input_ids.roll(-1, 1).eq(next_vals).cumprod(1)\n",
    "        \n",
    "        n_correct = test.sum(1).clamp(0,n_adds)\n",
    "        best_guess = n_correct.argmax()\n",
    "        \n",
    "#         for i in range(top_k):\n",
    "#             print(decode_obo(input_ids[i]), decode_obo(next_vals[i]), test[i].tolist(), n_correct[i].item())\n",
    "        \n",
    "        next_vals = next_vals[best_guess].unsqueeze(0)\n",
    "        n_correct = n_correct[best_guess]\n",
    "        embeds = embeds[best_guess].unsqueeze(0)\n",
    "        \n",
    "        print(\"Verification:\", decode_obo(input_ids[best_guess]), decode_obo(next_vals), n_correct.item())\n",
    "        \n",
    "        # Toss any wrong smallmodel outputs\n",
    "        next_vals = next_vals[:,:n_correct+1]\n",
    "        n_gen += n_correct+1\n",
    "        embeds = embeds[:,:n_correct+1]\n",
    "            \n",
    "        n_wrong = n_adds - n_correct\n",
    "        # kv updates are required for torch.compile with\n",
    "        # mode='reduce-overhead'\n",
    "        n_kv_s = []\n",
    "        for layer_idx in range(len(past_key_value_states)):\n",
    "            n_kv_s.append([])\n",
    "            for tensor_idx in range(2):\n",
    "                base = past_key_value_states[layer_idx][tensor_idx]\n",
    "                new = past_key_value_states[layer_idx][tensor_idx+2][best_guess].unsqueeze(0)\n",
    "                if n_wrong > 0:\n",
    "                    new = new[:,:,:-n_wrong]\n",
    "                base = torch.cat([base, new], dim=2)\n",
    "                n_kv_s[layer_idx].append(\n",
    "                    base.clone(memory_format=torch.contiguous_format).detach()\n",
    "                )\n",
    "                # torch._dynamo.mark_dynamic(n_kv_s[layer_idx][tensor_idx], 2)\n",
    "        kwargs[\"past_key_value_states\"] = n_kv_s\n",
    "\n",
    "        result = torch.cat((result, next_vals), dim=-1)\n",
    "        print(\"Updated output:\", decode_obo(result))\n",
    "        print()\n",
    "\n",
    "        next_input = next_vals[:,-1].unsqueeze(-1)\n",
    "\n",
    "    if not batched:\n",
    "        result = result[0]\n",
    "    return result, n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcfac5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7231, 0.2638, 0.2146, 0.1832, 0.1578, 0.1490, 0.0970, 0.0947, 0.0843,\n",
       "          0.0789],\n",
       "         [0.3288, 0.2745, 0.1755, 0.1384, 0.1326, 0.1156, 0.0887, 0.0745, 0.0739,\n",
       "          0.0622]]),\n",
       " tensor([[[2, 0, 0],\n",
       "          [2, 1, 0],\n",
       "          [2, 1, 1],\n",
       "          [2, 3, 0],\n",
       "          [5, 0, 0],\n",
       "          [2, 3, 1],\n",
       "          [2, 4, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 2, 0],\n",
       "          [2, 4, 1]],\n",
       " \n",
       "         [[5, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [3, 0, 0],\n",
       "          [5, 1, 0],\n",
       "          [4, 0, 0],\n",
       "          [1, 1, 0],\n",
       "          [2, 0, 0],\n",
       "          [5, 2, 0],\n",
       "          [3, 1, 0],\n",
       "          [1, 2, 0]]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANY-LENGTH VERSION\n",
    "\n",
    "def get_topk_tree(logits, k=50):\n",
    "    # probs: b h v\n",
    "    n_adds = logits.size(1)\n",
    "    probs = logits.softmax(2)\n",
    "    # Add a no-token option to each head\n",
    "    probs = torch.cat([torch.ones(probs.size(0),probs.size(1),1), probs], dim=2) # b h 6\n",
    "    probtable = torch.ones(*([probs.size(0)]+[probs.size(2)]*n_adds)) # b 6 6 6\n",
    "    # Populate probability table\n",
    "    for i in range(n_adds):\n",
    "        dimlist = [-1]+[1]*n_adds\n",
    "        dimlist[i+1] = probtable.size(i+1)\n",
    "        probtable *= probs[:,i].view(dimlist)\n",
    "    # Zero out impossible entries (i.e. nil nil token)\n",
    "    psize = probtable.size()\n",
    "    causal = torch.ones(psize[-1],psize[-1]) # 6 6\n",
    "    causal[0,1:].zero_()\n",
    "    for i in range(n_adds-1):\n",
    "        probtable *= causal.view(*(list(causal.size()) + [1]*(i)))\n",
    "    probtable = probtable.view(psize[0],-1)\n",
    "    # Zero out all-nil option\n",
    "    probtable[:,0].zero_()\n",
    "    # Fetch top-k most probable tree nodes\n",
    "    v,i = probtable.topk(k, dim=1) # b k\n",
    "    i = torch.stack(torch.unravel_index(i, psize[1:]), 1).permute(0,2,1)\n",
    "    # v: b k\n",
    "    # i: b k h\n",
    "    return v,i\n",
    "\n",
    "get_topk_tree(torch.randn(2,3,5), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ff10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbcdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ca86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b8b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d39218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53790f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4165d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculation: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7, 52,  6, 24,  6]])\n",
      "Verification: tensor([[ 52,   6,  24,   6, 108]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108]])\n",
      "\n",
      "Speculation: tensor([[108,   0,   0,   0,   0]])\n",
      "Verification: tensor([[121,  22,  22,  22, 118]]) 0\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121]])\n",
      "\n",
      "Speculation: tensor([[121,  24,  35,  73,  60]])\n",
      "Verification: tensor([[24, 35, 73, 60, 75]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75]])\n",
      "\n",
      "Speculation: tensor([[75, 28, 24, 60,  0]])\n",
      "Verification: tensor([[ 28,  24,  60, 112,  35]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112]])\n",
      "\n",
      "Speculation: tensor([[112, 118,  73,  92,   0]])\n",
      "Verification: tensor([[118,  73,  92, 124,  69]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124]])\n",
      "\n",
      "Speculation: tensor([[124,  96]])\n",
      "Verification: tensor([[96,  6]]) 1\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124,  96,\n",
      "           6]])\n",
      "\n",
      "0.11324882507324219\n",
      "Steps: 6\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "out, steps = speculative_generate(model, inp, 20, 20)\n",
    "print(time.time()-start)\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5601e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(out)):\n",
    "    assert out[0,i]==oracle[0,i], i\n",
    "print(\"Exact match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbox_oracle(inp):\n",
    "    out = oracle[:,inp.size(1):inp.size(1)+4].clone()\n",
    "    nzero = torch.randint(5,(1,))\n",
    "    if nzero > 0:\n",
    "        out[:,-nzero:] = 0\n",
    "    return out, min(oracle.size(1), inp.size(1)+4) - inp.size(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
