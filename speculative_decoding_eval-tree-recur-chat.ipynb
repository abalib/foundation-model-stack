{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee60d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch # was 20230913, now 20231013\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d529c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.models.llama import convert_hf_llama\n",
    "from transformers import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc2f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.models import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47561549",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(\n",
    "    \"llama\",\n",
    "    \"7b\",\n",
    "    model_path=\"../../../llama_weights/7B-F/\",\n",
    "    device_type=\"cpu\",\n",
    "    source=\"meta\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57035028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.utils.generation import generate\n",
    "from transformers import AutoTokenizer\n",
    "t = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "vinv = {v:k for k,v in t.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5616faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today']\n"
     ]
    }
   ],
   "source": [
    "inp = t(\"Yesterday is history, tomorrow is a mystery, today\")[\"input_ids\"]\n",
    "print([vinv[x] for x in inp])\n",
    "inp = torch.IntTensor(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "762260eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> Yesterday is history, tomorrow is a mystery, today is a gift.\\nThat's why they call it the present.\\n\\nThis quote is a reminder to appreciate the present moment and not\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle = generate(model, inp, 30, 30, do_sample=False, use_cache=True)\n",
    "t.decode(oracle.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6fd98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Speculator(n_heads=3)\n",
    "test.load_state_dict(torch.load(\"../../../specu_recur_n2.pth\", map_location=\"cpu\")[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08810d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887119872"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in test.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aecabfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: ['▁today', '▁is', '▁a', '▁day'] ['▁is', '▁a', '▁gift', '▁of'] 2\n",
      "Updated output: ['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today', '▁is', '▁a', '▁gift']\n",
      "\n",
      "Verification: ['▁gift', '.', '<0x0A>', 'This'] ['.', '<0x0A>', 'That', '▁quote'] 2\n",
      "Updated output: ['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today', '▁is', '▁a', '▁gift', '.', '<0x0A>', 'That']\n",
      "\n",
      "Verification: ['That', \"'\", 's', '▁why'] [\"'\", 's', '▁why', '▁they'] 3\n",
      "Updated output: ['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today', '▁is', '▁a', '▁gift', '.', '<0x0A>', 'That', \"'\", 's', '▁why', '▁they']\n",
      "\n",
      "Verification: ['▁they', '▁call', '▁it', '▁the'] ['▁call', '▁it', '▁the', '▁present'] 3\n",
      "Updated output: ['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today', '▁is', '▁a', '▁gift', '.', '<0x0A>', 'That', \"'\", 's', '▁why', '▁they', '▁call', '▁it', '▁the', '▁present']\n",
      "\n",
      "Verification: ['▁present', '.', '<0x0A>', 'G'] ['.', '<0x0A>', '<0x0A>', 'ift'] 2\n",
      "Updated output: ['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today', '▁is', '▁a', '▁gift', '.', '<0x0A>', 'That', \"'\", 's', '▁why', '▁they', '▁call', '▁it', '▁the', '▁present', '.', '<0x0A>', '<0x0A>']\n",
      "\n",
      "Verification: ['<0x0A>', 'This', '▁quote', '▁is'] ['This', '▁quote', '▁is', '▁a'] 3\n",
      "Updated output: ['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today', '▁is', '▁a', '▁gift', '.', '<0x0A>', 'That', \"'\", 's', '▁why', '▁they', '▁call', '▁it', '▁the', '▁present', '.', '<0x0A>', '<0x0A>', 'This', '▁quote', '▁is', '▁a']\n",
      "\n",
      "Verification: ['▁a', '▁rem', 'inder', '▁that'] ['▁rem', 'inder', '▁to', '▁the'] 2\n",
      "Updated output: ['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today', '▁is', '▁a', '▁gift', '.', '<0x0A>', 'That', \"'\", 's', '▁why', '▁they', '▁call', '▁it', '▁the', '▁present', '.', '<0x0A>', '<0x0A>', 'This', '▁quote', '▁is', '▁a', '▁rem', 'inder', '▁to']\n",
      "\n",
      "Verification: ['▁to', '▁appreciate', '▁the', '▁importance'] ['▁appreciate', '▁the', '▁present', '▁of'] 2\n",
      "Updated output: ['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today', '▁is', '▁a', '▁gift', '.', '<0x0A>', 'That', \"'\", 's', '▁why', '▁they', '▁call', '▁it', '▁the', '▁present', '.', '<0x0A>', '<0x0A>', 'This', '▁quote', '▁is', '▁a', '▁rem', 'inder', '▁to', '▁appreciate', '▁the', '▁present']\n",
      "\n",
      "Verification: ['▁present', '▁moment', '▁and', '▁to'] ['▁moment', '▁and', '▁not', '▁not'] 2\n",
      "Updated output: ['<s>', '▁Y', 'esterday', '▁is', '▁history', ',', '▁tom', 'orrow', '▁is', '▁a', '▁mystery', ',', '▁today', '▁is', '▁a', '▁gift', '.', '<0x0A>', 'That', \"'\", 's', '▁why', '▁they', '▁call', '▁it', '▁the', '▁present', '.', '<0x0A>', '<0x0A>', 'This', '▁quote', '▁is', '▁a', '▁rem', 'inder', '▁to', '▁appreciate', '▁the', '▁present', '▁moment', '▁and', '▁not']\n",
      "\n",
      "\n",
      "Steps: 9\n"
     ]
    }
   ],
   "source": [
    "out, steps = speculative_generate(model, inp, test, 30, 30, top_k=25, threshes=[10,3,2])\n",
    "print()\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eeebbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.modules.layernorm import LayerNormParameterized\n",
    "\n",
    "\n",
    "class Speculator(nn.Module):\n",
    "    def __init__(self, emb_dim=4096, vocab_size=32000, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.nheads = n_heads\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vsize = vocab_size\n",
    "        self.emb = nn.ModuleList([nn.Embedding(vocab_size, emb_dim) for _ in range(n_heads)])\n",
    "        self.proj = nn.ModuleList([nn.Linear(emb_dim * 2, emb_dim, bias=False) for _ in range(n_heads)])\n",
    "        self.head = nn.ModuleList([nn.Linear(emb_dim, vocab_size, bias=False) for _ in range(n_heads)])\n",
    "        self.ln = nn.ModuleList(\n",
    "            [LayerNormParameterized(emb_dim, elementwise_shift=True, elementwise_scale=True) for _ in range(n_heads)]\n",
    "        )\n",
    "        self.a = nn.GELU()\n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding) or isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, 0, 1 / self.emb_dim**0.5)\n",
    "            elif isinstance(m, LayerNormParameterized):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def generate_tree(self, state, ind, topk=[5,4,3], k=25):\n",
    "        # state: b 1 d\n",
    "        # ind: b 1\n",
    "        b = state.size(0)\n",
    "        out = torch.LongTensor(b,1,0) # b k h\n",
    "        log_probs = torch.zeros(b,1) # b k\n",
    "        assert len(topk)==self.nheads\n",
    "        for i in range(self.nheads):\n",
    "            z = self.emb[i](ind) # b k d\n",
    "            z = torch.cat([state, z], dim=2) # b k 2d\n",
    "            state = self.a(self.ln[i](self.proj[i](z))) # b k d\n",
    "            probs = F.log_softmax(self.head[i](state), dim=2) # b k v\n",
    "            probs, preds = probs.topk(topk[i], dim=2) # b k k'\n",
    "            out = out.unsqueeze(2).expand(-1,-1,topk[i],-1) # b k k' h\n",
    "            out = torch.cat([out, preds.unsqueeze(3)], dim=3) # b k k' h+1\n",
    "            \n",
    "            # Prep for next round\n",
    "            out = out.view(b, -1, i+1) # b kk' h+1\n",
    "            state = state.unsqueeze(2).expand(-1,-1,topk[i],-1) # b k k' d\n",
    "            state = state.reshape(b, -1, state.size(3)) # b kk' d\n",
    "            ind = preds.view(b, -1) # b kk'\n",
    "            log_probs = log_probs.unsqueeze(2).expand(b,-1,topk[i]) # b k k'\n",
    "            log_probs = log_probs.add(probs).reshape(b, -1) # b kk'\n",
    "            \n",
    "        best_guesses = log_probs.topk(k, dim=1)[1] # b k\n",
    "        \n",
    "        return out.gather(1, best_guesses.unsqueeze(2).expand(-1,-1,self.nheads)) # b k h\n",
    "            \n",
    "\n",
    "    def forward(self, state, inds):\n",
    "        # state: b n d\n",
    "        # inds: b n+2 (..., pred token, n+2, n+3)\n",
    "        out = []\n",
    "        for i in range(self.nheads):\n",
    "            h_inds = inds[:, i : i + state.size(1)]\n",
    "            z = self.emb[i](h_inds)  # b n d\n",
    "            z = torch.cat([state, z], dim=2)  # b n 2d\n",
    "            state = self.a(self.ln[i](self.proj[i](z)))  # b n d\n",
    "            out.append(self.head[i](state))  # b n v\n",
    "        return torch.stack(out, dim=0)  # h b n v\n",
    "\n",
    "    \n",
    "def decode_obo(x):\n",
    "    return [vinv[z] for z in x.squeeze().tolist()]\n",
    "\n",
    "\n",
    "def speculative_generate(\n",
    "    model: Union[Callable, torch.nn.Module],\n",
    "    input_ids: torch.LongTensor,\n",
    "    smallmodel: torch.nn.Module,\n",
    "    max_seq_len: int = 2048,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 24,\n",
    "    num_beams: int = 1,\n",
    "    threshes = [4,3,2]\n",
    "):\n",
    "    do_sample = False\n",
    "    batched = False\n",
    "    if num_beams != 1:\n",
    "        raise NotImplementedError(\"generate() does yet not support beam search\")\n",
    "    if type(input_ids) == torch.Tensor:\n",
    "        if input_ids.dim() != 1:\n",
    "            batched = True\n",
    "    else:\n",
    "        raise RuntimeError(\"generate() requires a tensor of token ids as the prefix\")\n",
    "\n",
    "    if not batched:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    result = input_ids\n",
    "    next_input = input_ids\n",
    "    kwargs = dict()\n",
    "    kwargs[\"past_key_value_states\"] = None\n",
    "    kwargs[\"use_cache\"] = True\n",
    "\n",
    "    output = model(input_ids[:,:-1], include_embeds=True, **kwargs)\n",
    "    _, past_key_value_states, embeds = output\n",
    "    embeds = embeds[:,-1:]\n",
    "    kwargs[\"past_key_value_states\"] = past_key_value_states\n",
    "    next_input = next_input[:,-1:]\n",
    "    \n",
    "    n_gen = 0\n",
    "    n_steps = 0\n",
    "    n_kv_s = past_key_value_states\n",
    "    while n_gen < max_new_tokens:\n",
    "        n_steps += 1\n",
    "        input_ids = next_input[:, -max_seq_len:]\n",
    "        \n",
    "        adds = smallmodel.generate_tree(embeds, input_ids, threshes, top_k)\n",
    "#         for i in range(adds.size(1)):\n",
    "#             print(decode_obo(adds[0,i]))\n",
    "        \n",
    "        n_adds = smallmodel.nheads\n",
    "#         probs = smallmodel(embeds, input_ids).squeeze(1) # b h v\n",
    "#         probs, topk = probs.topk(max(threshes), dim=2) # b h 5\n",
    "#         for i in range(n_adds):\n",
    "#             print(f\"Topk@{i+1}:\", decode_obo(topk[0,i]))\n",
    "        \n",
    "#         # Build probability table\n",
    "#         topk_v, topk_i = get_topk_tree(probs, top_k, threshes)\n",
    "        \n",
    "#         # Assemble batch of tree branches\n",
    "#         adds = topk.gather(2, topk_i).transpose(1,2) # b k h\n",
    "        adds = adds[0] # For now, non-batching and take only first b entry\n",
    "        input_ids = torch.cat([input_ids.expand(top_k,1), adds], dim=-1) \n",
    "#         print(\"Speculations:\")\n",
    "#         for i in range(top_k):\n",
    "#             print(decode_obo(input_ids[i]))\n",
    "        \n",
    "        mask = torch.ones(input_ids.size(1),input_ids.size(1)+n_kv_s[0][0].size(2))\n",
    "        mask = mask.tril(diagonal=mask.size(1)-mask.size(0))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0).log()\n",
    "        \n",
    "#         input_ids = input_ids[0].unsqueeze(0).expand(25,-1)\n",
    "        \n",
    "        output = model.forward(input_ids, include_embeds=True, mask=mask, **kwargs)\n",
    "        \n",
    "        logits, past_key_value_states, embeds = output\n",
    "        logits = logits[:, -n_adds-1:, :]\n",
    "\n",
    "        if do_sample:\n",
    "            # get logits from last value in sequence and scale\n",
    "#             logits = logits / temperature\n",
    "#             if top_k:\n",
    "#                 v, _ = torch.topk(logits, top_k)\n",
    "#                 logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "#             probs = F.softmax(logits, dim=-1)\n",
    "#             next_val = torch.multinomial(probs, num_samples=1)\n",
    "            assert False\n",
    "        else:\n",
    "            next_vals = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Check correctness of smallmodel predictions\n",
    "        test = input_ids.roll(-1, 1).eq(next_vals).cumprod(1)\n",
    "        \n",
    "        n_correct = test.sum(1).clamp(0,n_adds)\n",
    "        best_guess = n_correct.argmax()\n",
    "        \n",
    "#         for i in range(top_k):\n",
    "#             print(decode_obo(input_ids[i]), decode_obo(next_vals[i]), test[i].tolist(), n_correct[i].item())\n",
    "        \n",
    "        next_vals = next_vals[best_guess].unsqueeze(0)\n",
    "        n_correct = n_correct[best_guess]\n",
    "        embeds = embeds[best_guess].unsqueeze(0)\n",
    "        \n",
    "        print(\"Verification:\", decode_obo(input_ids[best_guess]), decode_obo(next_vals), n_correct.item())\n",
    "        \n",
    "        # Toss any wrong smallmodel outputs\n",
    "        next_vals = next_vals[:,:n_correct+1]\n",
    "        n_gen += n_correct+1\n",
    "        embeds = embeds[:,n_correct].unsqueeze(1)\n",
    "            \n",
    "        n_wrong = n_adds - n_correct\n",
    "        # kv updates are required for torch.compile with\n",
    "        # mode='reduce-overhead'\n",
    "        n_kv_s = []\n",
    "        for layer_idx in range(len(past_key_value_states)):\n",
    "            n_kv_s.append([])\n",
    "            for tensor_idx in range(2):\n",
    "                base = past_key_value_states[layer_idx][tensor_idx]\n",
    "                new = past_key_value_states[layer_idx][tensor_idx+2][best_guess].unsqueeze(0)\n",
    "                if n_wrong > 0:\n",
    "                    new = new[:,:,:-n_wrong]\n",
    "                base = torch.cat([base, new], dim=2)\n",
    "                n_kv_s[layer_idx].append(\n",
    "                    base.clone(memory_format=torch.contiguous_format).detach()\n",
    "                )\n",
    "                # torch._dynamo.mark_dynamic(n_kv_s[layer_idx][tensor_idx], 2)\n",
    "        kwargs[\"past_key_value_states\"] = n_kv_s\n",
    "\n",
    "        result = torch.cat((result, next_vals), dim=-1)\n",
    "        print(\"Updated output:\", decode_obo(result))\n",
    "        print()\n",
    "\n",
    "        next_input = next_vals[:,-1].unsqueeze(-1)\n",
    "\n",
    "    if not batched:\n",
    "        result = result[0]\n",
    "    return result, n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5698c5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 40.92\n",
      "5 36.18\n",
      "10 34.325\n",
      "25 33.585\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "test = torch.load(\"../../../specu_recur_n2_shortgen_scores.pth\")\n",
    "\n",
    "for k in test.keys():\n",
    "    steps = sum(test[k])/len(test[k])\n",
    "    print(k, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcfac5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7231, 0.2638, 0.2146, 0.1832, 0.1578, 0.1490, 0.0970, 0.0947, 0.0843,\n",
       "          0.0789],\n",
       "         [0.3288, 0.2745, 0.1755, 0.1384, 0.1326, 0.1156, 0.0887, 0.0745, 0.0739,\n",
       "          0.0622]]),\n",
       " tensor([[[2, 0, 0],\n",
       "          [2, 1, 0],\n",
       "          [2, 1, 1],\n",
       "          [2, 3, 0],\n",
       "          [5, 0, 0],\n",
       "          [2, 3, 1],\n",
       "          [2, 4, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 2, 0],\n",
       "          [2, 4, 1]],\n",
       " \n",
       "         [[5, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [3, 0, 0],\n",
       "          [5, 1, 0],\n",
       "          [4, 0, 0],\n",
       "          [1, 1, 0],\n",
       "          [2, 0, 0],\n",
       "          [5, 2, 0],\n",
       "          [3, 1, 0],\n",
       "          [1, 2, 0]]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANY-LENGTH VERSION\n",
    "\n",
    "def get_topk_tree(logits, k=50):\n",
    "    # probs: b h v\n",
    "    n_adds = logits.size(1)\n",
    "    probs = logits.softmax(2)\n",
    "    # Add a no-token option to each head\n",
    "    probs = torch.cat([torch.ones(probs.size(0),probs.size(1),1), probs], dim=2) # b h 6\n",
    "    probtable = torch.ones(*([probs.size(0)]+[probs.size(2)]*n_adds)) # b 6 6 6\n",
    "    # Populate probability table\n",
    "    for i in range(n_adds):\n",
    "        dimlist = [-1]+[1]*n_adds\n",
    "        dimlist[i+1] = probtable.size(i+1)\n",
    "        probtable *= probs[:,i].view(dimlist)\n",
    "    # Zero out impossible entries (i.e. nil nil token)\n",
    "    psize = probtable.size()\n",
    "    causal = torch.ones(psize[-1],psize[-1]) # 6 6\n",
    "    causal[0,1:].zero_()\n",
    "    for i in range(n_adds-1):\n",
    "        probtable *= causal.view(*(list(causal.size()) + [1]*(i)))\n",
    "    probtable = probtable.view(psize[0],-1)\n",
    "    # Zero out all-nil option\n",
    "    probtable[:,0].zero_()\n",
    "    # Fetch top-k most probable tree nodes\n",
    "    v,i = probtable.topk(k, dim=1) # b k\n",
    "    i = torch.stack(torch.unravel_index(i, psize[1:]), 1).permute(0,2,1)\n",
    "    # v: b k\n",
    "    # i: b k h\n",
    "    return v,i\n",
    "\n",
    "get_topk_tree(torch.randn(2,3,5), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ff10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbcdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ca86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b8b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d39218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53790f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4165d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculation: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7, 52,  6, 24,  6]])\n",
      "Verification: tensor([[ 52,   6,  24,   6, 108]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108]])\n",
      "\n",
      "Speculation: tensor([[108,   0,   0,   0,   0]])\n",
      "Verification: tensor([[121,  22,  22,  22, 118]]) 0\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121]])\n",
      "\n",
      "Speculation: tensor([[121,  24,  35,  73,  60]])\n",
      "Verification: tensor([[24, 35, 73, 60, 75]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75]])\n",
      "\n",
      "Speculation: tensor([[75, 28, 24, 60,  0]])\n",
      "Verification: tensor([[ 28,  24,  60, 112,  35]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112]])\n",
      "\n",
      "Speculation: tensor([[112, 118,  73,  92,   0]])\n",
      "Verification: tensor([[118,  73,  92, 124,  69]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124]])\n",
      "\n",
      "Speculation: tensor([[124,  96]])\n",
      "Verification: tensor([[96,  6]]) 1\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124,  96,\n",
      "           6]])\n",
      "\n",
      "0.11324882507324219\n",
      "Steps: 6\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "out, steps = speculative_generate(model, inp, 20, 20)\n",
    "print(time.time()-start)\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5601e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(out)):\n",
    "    assert out[0,i]==oracle[0,i], i\n",
    "print(\"Exact match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbox_oracle(inp):\n",
    "    out = oracle[:,inp.size(1):inp.size(1)+4].clone()\n",
    "    nzero = torch.randint(5,(1,))\n",
    "    if nzero > 0:\n",
    "        out[:,-nzero:] = 0\n",
    "    return out, min(oracle.size(1), inp.size(1)+4) - inp.size(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
