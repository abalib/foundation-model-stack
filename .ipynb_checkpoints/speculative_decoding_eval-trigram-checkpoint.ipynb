{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee60d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d529c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.models.llama import LLaMAConfig, LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1504b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = LLaMAConfig(32000, 4096, 1e-6, 32, 0, 32)\n",
    "model = LLaMA(modelc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4558b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(\"../../../llama_7b_ckp.pth\")['model_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce3e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = list(d.keys())\n",
    "for key in keylist:\n",
    "    if \"dec_process\" in key:\n",
    "        value = d.pop(key)\n",
    "        fields = key.split(\".\")\n",
    "        fields[0] = \"layers\"\n",
    "        d[\".\".join(fields)] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83097771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['rope.freqs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(d, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57035028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.utils.generation import generate\n",
    "from transformers import AutoTokenizer\n",
    "t = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "vinv = {v:k for k,v in t.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5616faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?']\n"
     ]
    }
   ],
   "source": [
    "inp = t(\"Hello! How are you today?\")[\"input_ids\"]\n",
    "print([vinv[x] for x in inp])\n",
    "inp = torch.IntTensor(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762260eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Hello! How are you today? I hope you are doing well. I am doing well. I am happy to be here with you'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle = generate(model, inp, 20, 20, do_sample=False, use_cache=True)\n",
    "t.decode(oracle.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49ebf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = torch.load(\"../../../cc123_trigram.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aecabfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk@1: ['<0x0A>', '▁What', '▁I', '▁How', '▁Call']\n",
      "Topk@2: ['The', 'A', 'We', 'I', 'If']\n",
      "Topk@3: ['▁', '▁first', '▁following', '▁best', '▁new']\n",
      "Speculation: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '<0x0A>', 'The', '▁']\n",
      "Verification: ['▁I', 'I', '▁weather', '2'] 0\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I']\n",
      "\n",
      "Topk@1: ['’', \"'\", '▁have', '▁am', '▁don']\n",
      "Topk@2: ['m', 've', 'll', 'd', 'M']\n",
      "Topk@3: ['▁not', '▁a', '▁going', '▁sure', '▁so']\n",
      "Speculation: ['▁I', '’', 'm', '▁not']\n",
      "Verification: ['▁hope', 'm', '▁doing', '▁sure'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope']\n",
      "\n",
      "Topk@1: ['▁you', '▁to', '▁that', '▁this', '▁it']\n",
      "Topk@2: ['▁enjoy', '▁will', '’', '▁are', '▁have']\n",
      "Topk@3: ['▁the', '▁your', '▁this', '▁a', '▁it']\n",
      "Speculation: ['▁hope', '▁you', '▁enjoy', '▁the']\n",
      "Verification: ['▁you', '▁are', '▁this', '▁video'] 1\n",
      "Acc@5: 2\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are']\n",
      "\n",
      "Topk@1: ['▁looking', '▁not', '▁a', '▁in', '▁interested']\n",
      "Topk@2: ['▁for', '▁to', '▁at', '▁forward', '▁into']\n",
      "Topk@3: ['▁a', '.', '▁the', '▁an', ',']\n",
      "Speculation: ['▁are', '▁looking', '▁for', '▁a']\n",
      "Verification: ['▁doing', '▁forward', '▁some', '▁new'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing']\n",
      "\n",
      "Topk@1: ['.', ',', '▁a', '▁the', '▁and']\n",
      "Topk@2: ['<0x0A>', '▁The', '▁I', '▁We', '▁It']\n",
      "Topk@3: ['The', 'In', '“', 'We', 'I']\n",
      "Speculation: ['▁doing', '.', '<0x0A>', 'The']\n",
      "Verification: ['▁well', '▁I', 'I', '▁weather'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well']\n",
      "\n",
      "Topk@1: ['.', ',', '▁and', '▁in', '▁on']\n",
      "Topk@2: ['<0x0A>', '▁The', '▁I', '▁It', '▁We']\n",
      "Topk@3: ['The', 'In', '“', 'We', 'I']\n",
      "Speculation: ['▁well', '.', '<0x0A>', 'The']\n",
      "Verification: ['.', '▁I', 'I', '▁weather'] 1\n",
      "Acc@5: 2\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I']\n",
      "\n",
      "Topk@1: ['▁have', '’', '▁am', \"'\", '▁was']\n",
      "Topk@2: ['▁been', '▁a', '▁to', '▁had', '▁no']\n",
      "Topk@3: ['▁a', '▁in', '▁made', '▁able', '▁the']\n",
      "Speculation: ['▁I', '▁have', '▁been', '▁a']\n",
      "Verification: ['▁am', '▁been', '▁busy', '▁bit'] 0\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am']\n",
      "\n",
      "Topk@1: ['▁a', '▁not', '▁very', '▁so', '▁going']\n",
      "Topk@2: ['▁', '▁very', '▁member', '▁big', '▁professional']\n",
      "Topk@3: ['1', '2', '3', '5', '4']\n",
      "Speculation: ['▁am', '▁a', '▁', '1']\n",
      "Verification: ['▁doing', '▁little', '2', '6'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing']\n",
      "\n",
      "Topk@1: ['▁a', '.', '▁my', '▁this', '▁the']\n",
      "Topk@2: ['▁lot', '▁great', '▁little', '▁good', '▁bit']\n",
      "Topk@3: ['▁of', '▁more', '▁to', '.', '▁about']\n",
      "Speculation: ['▁doing', '▁a', '▁lot', '▁of']\n",
      "Verification: ['▁well', '▁little', '▁better', '▁things'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well']\n",
      "\n",
      "Topk@1: ['.', ',', '▁and', '▁in', '▁on']\n",
      "Topk@2: ['<0x0A>', '▁The', '▁I', '▁It', '▁We']\n",
      "Topk@3: ['The', 'In', '“', 'We', 'I']\n",
      "Speculation: ['▁well', '.', '<0x0A>', 'The']\n",
      "Verification: ['.', '▁I', 'I', '▁weather'] 1\n",
      "Acc@5: 2\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I']\n",
      "\n",
      "Topk@1: ['▁have', '’', '▁am', \"'\", '▁was']\n",
      "Topk@2: ['▁been', '▁a', '▁to', '▁had', '▁no']\n",
      "Topk@3: ['▁a', '▁in', '▁made', '▁able', '▁the']\n",
      "Speculation: ['▁I', '▁have', '▁been', '▁a']\n",
      "Verification: ['▁am', '▁been', '▁busy', '▁bit'] 0\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am']\n",
      "\n",
      "Topk@1: ['▁a', '▁not', '▁very', '▁so', '▁going']\n",
      "Topk@2: ['▁', '▁very', '▁member', '▁big', '▁professional']\n",
      "Topk@3: ['1', '2', '3', '5', '4']\n",
      "Speculation: ['▁am', '▁a', '▁', '1']\n",
      "Verification: ['▁happy', '▁little', '2', '6'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy']\n",
      "\n",
      "Topk@1: ['▁to', '▁with', '▁that', '▁for', '▁I']\n",
      "Topk@2: ['▁help', '▁assist', '▁be', '▁answer', '▁have']\n",
      "Topk@3: ['▁you', '▁us', '▁them', '.', '▁the']\n",
      "Speculation: ['▁happy', '▁to', '▁help', '▁you']\n",
      "Verification: ['▁to', '▁be', '▁you', '▁with'] 1\n",
      "Acc@5: 2\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be']\n",
      "\n",
      "Topk@1: ['▁a', '▁the', '▁able', '▁in', '▁used']\n",
      "Topk@2: ['▁good', '▁great', '▁part', '▁little', '▁bit']\n",
      "Topk@3: ['▁idea', '▁time', '▁way', '▁thing', '▁faith']\n",
      "Speculation: ['▁be', '▁a', '▁good', '▁idea']\n",
      "Verification: ['▁here', '▁part', '▁friend', '.'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here']\n",
      "\n",
      "Topk@1: ['.', '▁to', '▁for', ',', '▁and']\n",
      "Topk@2: ['<0x0A>', '▁The', '▁I', '▁We', '▁If']\n",
      "Topk@3: ['The', 'In', '“', 'We', 'I']\n",
      "Speculation: ['▁here', '.', '<0x0A>', 'The']\n",
      "Verification: ['▁with', '▁I', 'I', '▁weather'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with']\n",
      "\n",
      "Topk@1: ['▁the', '▁us', '▁a', '▁you', '▁our']\n",
      "Topk@2: ['▁best', '▁help', '▁same', '▁most', '▁latest']\n",
      "Topk@3: ['▁way', '▁of', '▁possible', '▁experience', '▁in']\n",
      "Speculation: ['▁with', '▁the', '▁best', '▁way']\n",
      "Verification: ['▁you', '▁new', '▁of', '▁to'] 0\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with', '▁you']\n",
      "\n",
      "\n",
      "Steps: 16\n"
     ]
    }
   ],
   "source": [
    "out, steps = speculative_generate(model, inp, 20, 20)\n",
    "print()\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eeebbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_obo(x):\n",
    "    return [vinv[z] for z in x.squeeze().tolist()]\n",
    "    \n",
    "def speculative_generate(\n",
    "    model: Union[Callable, torch.nn.Module],\n",
    "    input_ids: torch.LongTensor,\n",
    "    max_seq_len: int = 2048,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 10,\n",
    "    num_beams: int = 1,\n",
    "):\n",
    "    do_sample = False\n",
    "    use_cache = True\n",
    "    batched = False\n",
    "    if num_beams != 1:\n",
    "        raise NotImplementedError(\"generate() does yet not support beam search\")\n",
    "    if type(input_ids) == torch.Tensor:\n",
    "        if input_ids.dim() != 1:\n",
    "            batched = True\n",
    "    else:\n",
    "        raise RuntimeError(\"generate() requires a tensor of token ids as the prefix\")\n",
    "\n",
    "    if not batched:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    result = input_ids\n",
    "    next_input = input_ids\n",
    "    kwargs = dict()\n",
    "    kwargs[\"past_key_value_states\"] = None\n",
    "    kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "    embeds = model(input_ids[:,:-1], include_embeds=True, **kwargs)\n",
    "    embeds = embeds[2] if use_cache else embeds[1]\n",
    "    n_gen = 0\n",
    "    n_steps = 0\n",
    "    while n_gen < max_new_tokens:\n",
    "        n_steps += 1\n",
    "        input_ids = next_input[:, -max_seq_len:]\n",
    "        \n",
    "        n_adds = 3\n",
    "        adds = torch.FloatTensor(torch.zeros(1,1,n_adds,32000))\n",
    "        tmp = result[0,-2:].tolist()\n",
    "        for i in range(n_adds):\n",
    "            pair = (tmp[0],tmp[1])\n",
    "            if pair not in trigram:\n",
    "                break\n",
    "            probs = trigram[pair]\n",
    "            imax = 0\n",
    "            pmax = 0\n",
    "            for ind,p in probs.items():\n",
    "                adds[:,:,i,ind] = p\n",
    "                if p > pmax:\n",
    "                    imax = ind\n",
    "                    pmax = p\n",
    "            tmp = (tmp[1], imax)\n",
    "\n",
    "#         adds = smallmodel(embeds[:,-1].unsqueeze(1))\n",
    "#         n_adds = smallmodel.nheads\n",
    "        \n",
    "        topk = adds.topk(5, dim=3)[1]\n",
    "        for i in range(n_adds):\n",
    "            print(f\"Topk@{i+1}:\", decode_obo(topk[0,0,i]))\n",
    "        adds = adds.argmax(3).squeeze(1) # b h\n",
    "        input_ids = torch.cat([input_ids, adds], dim=-1)\n",
    "        print(\"Speculation:\", decode_obo(input_ids))\n",
    "        output = model.forward(input_ids, include_embeds=True, **kwargs)\n",
    "        if use_cache:\n",
    "            logits, past_key_value_states, embeds = output\n",
    "        else:\n",
    "            logits, embeds = output\n",
    "        logits = logits[:, -n_adds-1:, :]\n",
    "\n",
    "        if do_sample:\n",
    "            # get logits from last value in sequence and scale\n",
    "#             logits = logits / temperature\n",
    "#             if top_k:\n",
    "#                 v, _ = torch.topk(logits, top_k)\n",
    "#                 logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "#             probs = F.softmax(logits, dim=-1)\n",
    "#             next_val = torch.multinomial(probs, num_samples=1)\n",
    "            assert False\n",
    "        else:\n",
    "            next_vals = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Check correctness of smallmodel predictions\n",
    "        n_correct = 0\n",
    "        while n_correct < n_adds and next_vals[0,n_correct] == input_ids[0,-n_adds+n_correct]:\n",
    "            n_correct += 1\n",
    "        print(\"Verification:\", decode_obo(next_vals), n_correct)\n",
    "        \n",
    "        k_correct = 0\n",
    "        while (\n",
    "            result.size(1)+k_correct < len(oracle) and \n",
    "            k_correct < n_adds and\n",
    "            oracle[result.size(1)+k_correct] in topk[0,0,k_correct]\n",
    "        ):\n",
    "            k_correct += 1\n",
    "        print(\"Acc@5:\", k_correct)\n",
    "        \n",
    "        # Toss any wrong smallmodel outputs\n",
    "        next_vals = next_vals[:,:n_correct+1]\n",
    "        n_gen += n_correct+1\n",
    "        embeds = embeds[:,:n_correct+1]\n",
    "            \n",
    "        n_wrong = n_adds - n_correct\n",
    "        if use_cache:\n",
    "            # kv updates are required for torch.compile with\n",
    "            # mode='reduce-overhead'\n",
    "            n_kv_s = []\n",
    "            for layer_idx in range(len(past_key_value_states)):\n",
    "                n_kv_s.append([])\n",
    "                for tensor_idx in range(len(past_key_value_states[layer_idx])):\n",
    "                    base = past_key_value_states[layer_idx][tensor_idx]\n",
    "                    if n_wrong > 0:\n",
    "                        base = base[:,:,:-n_wrong]\n",
    "                    n_kv_s[layer_idx].append(\n",
    "                        base.clone(memory_format=torch.contiguous_format).detach()\n",
    "                    )\n",
    "                    # torch._dynamo.mark_dynamic(n_kv_s[layer_idx][tensor_idx], 2)\n",
    "            kwargs[\"past_key_value_states\"] = n_kv_s\n",
    "\n",
    "        result = torch.cat((result, next_vals), dim=-1)\n",
    "        print(\"Updated output:\", decode_obo(result))\n",
    "        print()\n",
    "\n",
    "        if use_cache:\n",
    "            next_input = next_vals[:,-1].unsqueeze(-1)\n",
    "        else:\n",
    "            next_input = result\n",
    "\n",
    "    if not batched:\n",
    "        result = result[0]\n",
    "    return result, n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfac5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ff10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbcdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ca86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b8b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d39218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53790f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df741a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
