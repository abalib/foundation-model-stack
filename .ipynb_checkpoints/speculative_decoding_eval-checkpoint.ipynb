{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee60d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d529c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.models.llama import LLaMAConfig, LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1504b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = LLaMAConfig(32000, 4096, 1e-6, 32, 0, 32)\n",
    "model = LLaMA(modelc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4558b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(\"../../../llama_7b_ckp.pth\")['model_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce3e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = list(d.keys())\n",
    "for key in keylist:\n",
    "    if \"dec_process\" in key:\n",
    "        value = d.pop(key)\n",
    "        fields = key.split(\".\")\n",
    "        fields[0] = \"layers\"\n",
    "        d[\".\".join(fields)] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83097771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['rope.freqs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(d, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57035028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.utils.generation import generate\n",
    "from transformers import AutoTokenizer\n",
    "t = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "vinv = {v:k for k,v in t.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5616faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is']\n"
     ]
    }
   ],
   "source": [
    "inp = t(\"The largest ocean dwelling insect is\")[\"input_ids\"]\n",
    "print([vinv[x] for x in inp])\n",
    "inp = torch.IntTensor(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "762260eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> The largest ocean dwelling insect is the coconut crab. The largest specimen ever recorded was 17.5 inches'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle = generate(model, inp, 20, 20, do_sample=False, use_cache=True)\n",
    "t.decode(oracle.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dc6fd98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Speculator(n_heads=3)\n",
    "test.load_state_dict(torch.load(\"../../../specu_greedy.pth\", map_location=\"cpu\")[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "08810d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844107776"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in test.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "07b30a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = torch.load(\"../../../cc123_trigram.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "aecabfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk@1: ['▁the', '▁in', '▁is', '<0x0A>', '▁are']\n",
      "Topk@2: ['▁the', '▁world', '▁is', 'The', '<0x0A>']\n",
      "Topk@3: ['▁the', '▁is', ',', '<0x0A>', '.']\n",
      "Speculation: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁the', '▁the']\n",
      "Verification: ['▁the', '▁co', '▁giant', '▁giant'] 1\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co']\n",
      "\n",
      "Topk@1: ['▁of', '▁is', '-', '▁time', '2']\n",
      "Topk@2: ['▁the', '▁of', '▁is', '▁a', '0']\n",
      "Topk@3: ['▁the', '▁is', '▁of', '▁a', '▁to']\n",
      "Speculation: ['▁co', '▁of', '▁the', '▁the']\n",
      "Verification: ['con', '▁the', '▁genus', '▁giant'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con']\n",
      "\n",
      "Topk@1: ['pod', 'ut', 'fish', 'con', '▁sea']\n",
      "Topk@2: ['▁sp', ',', 'rab', 'orm', '.']\n",
      "Topk@3: [',', '.', '▁the', '▁and', '▁which']\n",
      "Speculation: ['con', 'pod', '▁sp', ',']\n",
      "Verification: ['ut', ',', 'ider', '▁which'] 0\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut']\n",
      "\n",
      "Topk@1: ['▁shell', '▁sh', '▁c', '▁sc', ',']\n",
      "Topk@2: ['oon', 'rab', ',', 'ell', 've']\n",
      "Topk@3: [',', '▁a', '▁is', '.', 'oon']\n",
      "Speculation: ['ut', '▁shell', 'oon', ',']\n",
      "Verification: ['▁c', '▁c', ',', '▁which'] 0\n",
      "Acc@5: 3\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c']\n",
      "\n",
      "Topk@1: ['rab', 'ob', '.', 'orp', 'fish']\n",
      "Topk@2: ['.', '<0x0A>', 'ler', 'le', 'er']\n",
      "Topk@3: ['.', '<0x0A>', '▁This', '▁a', '▁is']\n",
      "Speculation: ['▁c', 'rab', '.', '.']\n",
      "Verification: ['rab', '.', '▁The', 'The'] 2\n",
      "Acc@5: 2\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c', 'rab', '.', '▁The']\n",
      "\n",
      "Topk@1: ['The', '▁large', '▁is', '▁co', '▁c']\n",
      "Topk@2: ['▁is', '▁the', '▁co', '▁of', '▁a']\n",
      "Topk@3: ['▁is', '▁the', '▁a', '▁C', '▁c']\n",
      "Speculation: ['▁The', 'The', '▁is', '▁is']\n",
      "Verification: ['▁largest', '▁largest', '▁the', '▁a'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c', 'rab', '.', '▁The', '▁largest']\n",
      "\n",
      "Topk@1: ['▁of', '▁is', 'rab', '▁c', '▁known']\n",
      "Topk@2: ['▁is', '▁of', '▁the', 's', '▁are']\n",
      "Topk@3: ['▁is', '▁the', 'rab', '▁largest', '▁of']\n",
      "Speculation: ['▁largest', '▁of', '▁is', '▁is']\n",
      "Verification: ['▁spec', '▁the', '▁species', '▁the'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c', 'rab', '.', '▁The', '▁largest', '▁spec']\n",
      "\n",
      "Topk@1: ['▁can', 'rab', '▁of', '▁ever', '▁c']\n",
      "Topk@2: ['▁ever', '▁is', '▁can', '▁the', '▁in']\n",
      "Topk@3: ['▁ever', '▁is', 'rab', '▁the', '▁in']\n",
      "Speculation: ['▁spec', '▁can', '▁ever', '▁ever']\n",
      "Verification: ['imen', '▁we', '▁reach', '▁reach'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c', 'rab', '.', '▁The', '▁largest', '▁spec', 'imen']\n",
      "\n",
      "Topk@1: ['▁ever', '▁found', '▁I', 'ens', '▁was']\n",
      "Topk@2: ['▁found', '▁', '▁in', '▁have', '▁the']\n",
      "Topk@3: ['▁', '▁the', '▁found', '▁in', '1']\n",
      "Speculation: ['imen', '▁ever', '▁found', '▁']\n",
      "Verification: ['▁ever', '▁recorded', '▁was', '1'] 1\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c', 'rab', '.', '▁The', '▁largest', '▁spec', 'imen', '▁ever', '▁recorded']\n",
      "\n",
      "Topk@1: ['▁was', '▁is', 'ighed', '▁we', ',']\n",
      "Topk@2: ['▁was', '▁', '▁the', '▁is', '▁a']\n",
      "Topk@3: ['▁', '▁was', '1', '▁is', '▁the']\n",
      "Speculation: ['▁recorded', '▁was', '▁was', '▁']\n",
      "Verification: ['▁was', '▁', 'hed', '1'] 1\n",
      "Acc@5: 3\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c', 'rab', '.', '▁The', '▁largest', '▁spec', 'imen', '▁ever', '▁recorded', '▁was', '▁']\n",
      "\n",
      "Topk@1: ['▁', '1', '2', '3', '▁in']\n",
      "Topk@2: ['1', '▁', '0', '2', '5']\n",
      "Topk@3: ['0', '▁feet', '▁inches', '▁', '.']\n",
      "Speculation: ['▁', '▁', '1', '0']\n",
      "Verification: ['1', '1', '7', '.'] 0\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c', 'rab', '.', '▁The', '▁largest', '▁spec', 'imen', '▁ever', '▁recorded', '▁was', '▁', '1']\n",
      "\n",
      "Topk@1: ['.', '0', '▁inches', '8', '5']\n",
      "Topk@2: ['.', '▁inches', '▁(', '▁in', 'cm']\n",
      "Topk@3: ['▁(', '5', '.', '▁in', '▁tall']\n",
      "Speculation: ['1', '.', '.', '▁(']\n",
      "Verification: ['7', '8', '8', '4'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c', 'rab', '.', '▁The', '▁largest', '▁spec', 'imen', '▁ever', '▁recorded', '▁was', '▁', '1', '7']\n",
      "\n",
      "Topk@1: ['.', '▁feet', '▁inches', '5', '7']\n",
      "Topk@2: ['5', '▁(', '.', '▁cm', '7']\n",
      "Topk@3: ['▁(', '▁and', '.', '▁m', '5']\n",
      "Speculation: ['7', '.', '5', '▁(']\n",
      "Verification: ['.', '5', '▁inches', '4'] 2\n",
      "Acc@5: 2\n",
      "Updated output: ['<s>', '▁The', '▁largest', '▁ocean', '▁dwell', 'ing', '▁insect', '▁is', '▁the', '▁co', 'con', 'ut', '▁c', 'rab', '.', '▁The', '▁largest', '▁spec', 'imen', '▁ever', '▁recorded', '▁was', '▁', '1', '7', '.', '5', '▁inches']\n",
      "\n",
      "\n",
      "Steps: 13\n"
     ]
    }
   ],
   "source": [
    "out, steps = speculative_generate(model, inp, test, 20, 20)\n",
    "print()\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0eeebbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.modules.layernorm import LayerNormParameterized\n",
    "\n",
    "class Speculator(nn.Module):\n",
    "    def __init__(self, emb_dim=4096, vocab_size=32000, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.nheads = n_heads\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vsize = vocab_size\n",
    "        self.w_in = nn.Parameter(torch.empty(emb_dim, int((emb_dim * 2.6875 * 2) // 256) * 256 * 2))  # d 2z\n",
    "        self.a = nn.GELU()\n",
    "        self.w_out = nn.Parameter(torch.empty(int((emb_dim * 2.6875 * 2) // 256) * 256, emb_dim * n_heads))  # z hd\n",
    "        self.ln = LayerNormParameterized(emb_dim, elementwise_shift=False, elementwise_scale=True)\n",
    "        self.head = nn.Parameter(torch.empty(n_heads, emb_dim, vocab_size))  # h d v\n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        nn.init.trunc_normal_(self.w_in, 0, (1 / 2.6875) ** (1 / 6) / self.emb_dim**0.5)\n",
    "        nn.init.trunc_normal_(self.w_out, 0, (1 / 2.6875) ** (1 / 6) / self.emb_dim**0.5)\n",
    "        nn.init.trunc_normal_(self.head, 0, 1 / self.emb_dim**0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: b n d\n",
    "        z, g = x.matmul(self.w_in).chunk(2, dim=2)\n",
    "        z = z * self.a(g)\n",
    "        z = z.matmul(self.w_out).view(x.size(0), x.size(1), self.nheads, self.emb_dim)  # b n h d\n",
    "        z = z + x.unsqueeze(2)\n",
    "        z = self.ln(z)\n",
    "        z = torch.einsum(\"bnhd,hdv->bnhv\", z, self.head)\n",
    "        return z # b n h v\n",
    "    \n",
    "def decode_obo(x):\n",
    "    return [vinv[z] for z in x.squeeze().tolist()]\n",
    "    \n",
    "def speculative_generate(\n",
    "    model: Union[Callable, torch.nn.Module],\n",
    "    input_ids: torch.LongTensor,\n",
    "    smallmodel: torch.nn.Module,\n",
    "    max_seq_len: int = 2048,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 10,\n",
    "    num_beams: int = 1,\n",
    "):\n",
    "    do_sample = False\n",
    "    use_cache = True\n",
    "    batched = False\n",
    "    if num_beams != 1:\n",
    "        raise NotImplementedError(\"generate() does yet not support beam search\")\n",
    "    if type(input_ids) == torch.Tensor:\n",
    "        if input_ids.dim() != 1:\n",
    "            batched = True\n",
    "    else:\n",
    "        raise RuntimeError(\"generate() requires a tensor of token ids as the prefix\")\n",
    "\n",
    "    if not batched:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    result = input_ids\n",
    "    next_input = input_ids\n",
    "    kwargs = dict()\n",
    "    kwargs[\"past_key_value_states\"] = None\n",
    "    kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "    embeds = model(input_ids[:,:-1], include_embeds=True, **kwargs)\n",
    "    embeds = embeds[2] if use_cache else embeds[1]\n",
    "    n_gen = 0\n",
    "    n_steps = 0\n",
    "    while n_gen < max_new_tokens:\n",
    "        n_steps += 1\n",
    "        input_ids = next_input[:, -max_seq_len:]\n",
    "        \n",
    "#         n_adds = 3\n",
    "#         adds = torch.FloatTensor(torch.zeros(1,1,n_adds,32000))\n",
    "#         tmp = result[0,-2:].tolist()\n",
    "#         for i in range(n_adds):\n",
    "#             pair = (tmp[0],tmp[1])\n",
    "#             if pair not in trigram:\n",
    "#                 break\n",
    "#             probs = trigram[pair]\n",
    "#             imax = 0\n",
    "#             pmax = 0\n",
    "#             for ind,p in probs.items():\n",
    "#                 adds[:,:,i,ind] = p\n",
    "#                 if p > pmax:\n",
    "#                     imax = ind\n",
    "#                     pmax = p\n",
    "#             tmp = (tmp[1], imax)\n",
    "\n",
    "        adds = smallmodel(embeds[:,-1].unsqueeze(1))\n",
    "        n_adds = smallmodel.nheads\n",
    "        \n",
    "        topk = adds.topk(5, dim=3)[1]\n",
    "        for i in range(n_adds):\n",
    "            print(f\"Topk@{i+1}:\", decode_obo(topk[0,0,i]))\n",
    "        adds = adds.argmax(3).squeeze(1) # b h\n",
    "        input_ids = torch.cat([input_ids, adds], dim=-1)\n",
    "        print(\"Speculation:\", decode_obo(input_ids))\n",
    "        output = model.forward(input_ids, include_embeds=True, **kwargs)\n",
    "        if use_cache:\n",
    "            logits, past_key_value_states, embeds = output\n",
    "        else:\n",
    "            logits, embeds = output\n",
    "        logits = logits[:, -n_adds-1:, :]\n",
    "\n",
    "        if do_sample:\n",
    "            # get logits from last value in sequence and scale\n",
    "#             logits = logits / temperature\n",
    "#             if top_k:\n",
    "#                 v, _ = torch.topk(logits, top_k)\n",
    "#                 logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "#             probs = F.softmax(logits, dim=-1)\n",
    "#             next_val = torch.multinomial(probs, num_samples=1)\n",
    "            assert False\n",
    "        else:\n",
    "            next_vals = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Check correctness of smallmodel predictions\n",
    "        n_correct = 0\n",
    "        while n_correct < n_adds and next_vals[0,n_correct] == input_ids[0,-n_adds+n_correct]:\n",
    "            n_correct += 1\n",
    "        print(\"Verification:\", decode_obo(next_vals), n_correct)\n",
    "        \n",
    "        k_correct = 0\n",
    "        while (\n",
    "            result.size(1)+k_correct < len(oracle) and \n",
    "            k_correct < n_adds and\n",
    "            oracle[result.size(1)+k_correct] in topk[0,0,k_correct]\n",
    "        ):\n",
    "            k_correct += 1\n",
    "        print(\"Acc@5:\", k_correct)\n",
    "        \n",
    "        # Toss any wrong smallmodel outputs\n",
    "        next_vals = next_vals[:,:n_correct+1]\n",
    "        n_gen += n_correct+1\n",
    "        embeds = embeds[:,:n_correct+1]\n",
    "            \n",
    "        n_wrong = n_adds - n_correct\n",
    "        if use_cache:\n",
    "            # kv updates are required for torch.compile with\n",
    "            # mode='reduce-overhead'\n",
    "            n_kv_s = []\n",
    "            for layer_idx in range(len(past_key_value_states)):\n",
    "                n_kv_s.append([])\n",
    "                for tensor_idx in range(len(past_key_value_states[layer_idx])):\n",
    "                    base = past_key_value_states[layer_idx][tensor_idx]\n",
    "                    if n_wrong > 0:\n",
    "                        base = base[:,:,:-n_wrong]\n",
    "                    n_kv_s[layer_idx].append(\n",
    "                        base.clone(memory_format=torch.contiguous_format).detach()\n",
    "                    )\n",
    "                    # torch._dynamo.mark_dynamic(n_kv_s[layer_idx][tensor_idx], 2)\n",
    "            kwargs[\"past_key_value_states\"] = n_kv_s\n",
    "\n",
    "        result = torch.cat((result, next_vals), dim=-1)\n",
    "        print(\"Updated output:\", decode_obo(result))\n",
    "        print()\n",
    "\n",
    "        if use_cache:\n",
    "            next_input = next_vals[:,-1].unsqueeze(-1)\n",
    "        else:\n",
    "            next_input = result\n",
    "\n",
    "    if not batched:\n",
    "        result = result[0]\n",
    "    return result, n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfac5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ff10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbcdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ca86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b8b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d39218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53790f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4165d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculation: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7, 52,  6, 24,  6]])\n",
      "Verification: tensor([[ 52,   6,  24,   6, 108]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108]])\n",
      "\n",
      "Speculation: tensor([[108,   0,   0,   0,   0]])\n",
      "Verification: tensor([[121,  22,  22,  22, 118]]) 0\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121]])\n",
      "\n",
      "Speculation: tensor([[121,  24,  35,  73,  60]])\n",
      "Verification: tensor([[24, 35, 73, 60, 75]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75]])\n",
      "\n",
      "Speculation: tensor([[75, 28, 24, 60,  0]])\n",
      "Verification: tensor([[ 28,  24,  60, 112,  35]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112]])\n",
      "\n",
      "Speculation: tensor([[112, 118,  73,  92,   0]])\n",
      "Verification: tensor([[118,  73,  92, 124,  69]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124]])\n",
      "\n",
      "Speculation: tensor([[124,  96]])\n",
      "Verification: tensor([[96,  6]]) 1\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124,  96,\n",
      "           6]])\n",
      "\n",
      "0.11324882507324219\n",
      "Steps: 6\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "out, steps = speculative_generate(model, inp, 20, 20)\n",
    "print(time.time()-start)\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5601e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(out)):\n",
    "    assert out[0,i]==oracle[0,i], i\n",
    "print(\"Exact match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbox_oracle(inp):\n",
    "    out = oracle[:,inp.size(1):inp.size(1)+4].clone()\n",
    "    nzero = torch.randint(5,(1,))\n",
    "    if nzero > 0:\n",
    "        out[:,-nzero:] = 0\n",
    "    return out, min(oracle.size(1), inp.size(1)+4) - inp.size(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
