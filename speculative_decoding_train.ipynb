{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee60d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d529c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.models.llama import LLaMAConfig, LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1504b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = LLaMAConfig(32000, 4096, 1e-6, 32, 0, 32)\n",
    "model = LLaMA(modelc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4558b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(\"../../../llama_7b_ckp.pth\")['model_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce3e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = list(d.keys())\n",
    "for key in keylist:\n",
    "    if \"dec_process\" in key:\n",
    "        value = d.pop(key)\n",
    "        fields = key.split(\".\")\n",
    "        fields[0] = \"layers\"\n",
    "        d[\".\".join(fields)] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83097771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['rope.freqs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(d, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57035028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fms.utils.generation import generate\n",
    "from transformers import AutoTokenizer\n",
    "t = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "vinv = {v:k for k,v in t.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a094b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?']\n",
      "['<s>', '▁Where', '▁will', '▁you', '▁be', '▁tom', 'orrow', '?']\n"
     ]
    }
   ],
   "source": [
    "inp = t(\"Hello! How are you today?\")[\"input_ids\"]\n",
    "inp2 = t(\"Where will you be tomorrow?\")[\"input_ids\"]\n",
    "inp = torch.stack([torch.IntTensor(inp),torch.IntTensor(inp2)])\n",
    "for line in inp:\n",
    "    print([vinv[x.item()] for x in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "762260eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4096]) <s> Hello! How are you today? How\n",
      "torch.Size([2, 9, 4096]) <s> Hello! How are you today? How is\n",
      "torch.Size([2, 10, 4096]) <s> Hello! How are you today? How is life\n",
      "torch.Size([2, 11, 4096]) <s> Hello! How are you today? How is life?\n",
      "torch.Size([2, 12, 4096]) <s> Hello! How are you today? How is life?\n",
      "\n",
      "torch.Size([2, 13, 4096]) <s> Hello! How are you today? How is life?\n",
      "I\n",
      "torch.Size([2, 14, 4096]) <s> Hello! How are you today? How is life?\n",
      "I hope\n",
      "torch.Size([2, 15, 4096]) <s> Hello! How are you today? How is life?\n",
      "I hope you\n",
      "['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁How', '▁is', '▁life', '?', '<0x0A>', 'I', '▁hope', '▁you']\n",
      "['<s>', '▁Where', '▁will', '▁you', '▁be', '▁tom', 'orrow', '?', '<0x0A>', 'In', '▁the', '▁', '1', '2', '▁days', '▁of']\n"
     ]
    }
   ],
   "source": [
    "out, kv, embeds = generate(model, inp, 8, 8, do_sample=True, use_cache=True)\n",
    "for line in out:\n",
    "    print([vinv[x] for x in line.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40f60dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁Hello',\n",
       " '!',\n",
       " '▁How',\n",
       " '▁are',\n",
       " '▁you',\n",
       " '▁today',\n",
       " '?',\n",
       " '▁How',\n",
       " '▁is',\n",
       " '▁life',\n",
       " '?',\n",
       " '<0x0A>',\n",
       " 'I',\n",
       " '▁hope',\n",
       " '▁you']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = [vinv[x] for x in out[0].tolist()]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c75028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁How', '▁is', '▁life', '?', '<0x0A>', 'I']\n",
      "['!', '▁How', '▁are', '▁you', '▁today', '?', '▁How', '▁is', '▁life', '?', '<0x0A>', 'I', '▁hope', '▁you']\n"
     ]
    }
   ],
   "source": [
    "e = line[:-1]\n",
    "t = line[1:]\n",
    "\n",
    "print(e[:-1])\n",
    "print(t[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d93728f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16]), torch.Size([2, 15, 4096]), torch.Size([2, 32, 15, 128]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size(), embeds.size(), kv[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00dbd9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 262144])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kv = torch.stack([torch.cat([x.transpose(1,2) for x in kv_], dim=2) for kv_ in kv], dim=2).flatten(2)\n",
    "new_kv.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55abe0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> Where will you be tomorrow? This year, I will be at the National Conference on Volunteer Engagement in Indianapolis, Indiana.\\nIf you're looking for ways to engage more volunteers on campus for your non-profit or for-profit business, you'll want to be there as well. Here are five reasons to attend:\\n1. The 2017 NCVAE is a great place to meet people.\\nIf you want to meet people from across the country, this is the perfect conference for you. You'll have the opportunity to meet other non-profits in person and learn what they'\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode(out[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0eeebbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model: Union[Callable, torch.nn.Module],\n",
    "    input_ids: torch.LongTensor,\n",
    "    max_seq_len: int = 2048,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 10,\n",
    "    do_sample: bool = True,\n",
    "    num_beams: int = 1,\n",
    "    use_cache: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    A trivial generate function that can be used for validation/testing in\n",
    "    cases where HF is not available.\n",
    "    We could add implementations for other types of generation, but this is\n",
    "    enough for making sure a model is working.\n",
    "    Does not implement batching nor beam search, but those could be added.\n",
    "\n",
    "    Args:\n",
    "        model: A function or nn.Module that takes a batch of input_ids and\n",
    "            returns logits\n",
    "        prefix: A tensor of token IDs.\n",
    "        max_seq_len: the sequence length of the model\n",
    "        max_new_tokens: max tokens to generate\n",
    "        temperature: temperature of softmax when sampling\n",
    "        top_k: only search among top k tokens\n",
    "        do_sample: multinomial sampling. False for greedy.\n",
    "        num_beams: TODO: support beam search\n",
    "        use_cache: requires that the model accept use_cache and\n",
    "            past_key_value_states args in forward method.\n",
    "    \"\"\"\n",
    "    batched = False\n",
    "    if num_beams != 1:\n",
    "        raise NotImplementedError(\"generate() does yet not support beam search\")\n",
    "    if type(input_ids) == torch.Tensor:\n",
    "        if input_ids.dim() != 1:\n",
    "            batched = True\n",
    "    else:\n",
    "        raise RuntimeError(\"generate() requires a tensor of token ids as the prefix\")\n",
    "\n",
    "    if not batched:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    embeds = None\n",
    "    result = input_ids\n",
    "    next_input = input_ids\n",
    "    kwargs = dict()\n",
    "    kwargs[\"past_key_value_states\"] = None\n",
    "    kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        input_ids = next_input[:, -max_seq_len:]\n",
    "        output = model.forward(input_ids, include_embeds=True, **kwargs)\n",
    "        if use_cache:\n",
    "            logits, past_key_value_states, z = output\n",
    "            # kv updates are required for torch.compile with\n",
    "            # mode='reduce-overhead'\n",
    "            n_kv_s = []\n",
    "            for layer_idx in range(len(past_key_value_states)):\n",
    "                n_kv_s.append([])\n",
    "                for tensor_idx in range(len(past_key_value_states[layer_idx])):\n",
    "                    n_kv_s[layer_idx].append(\n",
    "                        past_key_value_states[layer_idx][tensor_idx]\n",
    "                        .clone(memory_format=torch.contiguous_format)\n",
    "                        .detach()\n",
    "                    )\n",
    "                    # torch._dynamo.mark_dynamic(n_kv_s[layer_idx][tensor_idx], 2)\n",
    "            kwargs[\"past_key_value_states\"] = n_kv_s\n",
    "        else:\n",
    "            logits, z = output\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if do_sample:\n",
    "            # get logits from last value in sequence nad scale\n",
    "            logits = logits / temperature\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_val = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_val = torch.argmax(logits, dim=-1).unsqueeze(0).t()\n",
    "\n",
    "        result = torch.cat((result, next_val), dim=-1)\n",
    "        if embeds is None:\n",
    "            embeds = z\n",
    "        else:\n",
    "            embeds = torch.cat((embeds, z), dim=-2)\n",
    "        print(embeds.size(), t.decode(result[0].tolist()))\n",
    "\n",
    "        if use_cache:\n",
    "            next_input = next_val\n",
    "        else:\n",
    "            next_input = result\n",
    "\n",
    "    if not batched:\n",
    "        result = result[0]\n",
    "    return result, n_kv_s, embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfac5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ff10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbcdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ca86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b8b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d39218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53790f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df741a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
