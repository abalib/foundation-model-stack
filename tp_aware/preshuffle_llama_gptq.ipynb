{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/vllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from safetensors.torch import load_file, save_file, safe_open\n",
    "from transformers import AutoConfig\n",
    "\n",
    "torch.set_default_device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the conversion is model specific -- it works only for Llama-2\n",
    "# checkpoint_path = '/net/storage149/autofs/css22/cliu22/llama2_70bchat_gptq/gptq_output_act1_grp128_bluewiki'\n",
    "checkpoint_path = '/data/models/ibm/llama-2-70b-gptq/llama-70b-gptq-act1-grp128-tp-aware'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configs for debug assertions\n",
    "config = AutoConfig.from_pretrained(checkpoint_path)\n",
    "hidden_size = config.hidden_size\n",
    "intermediate_size = config.intermediate_size\n",
    "group_size = config.quantization_config['group_size']\n",
    "bits = config.quantization_config['bits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = load_file(os.path.join(checkpoint_path, 'model.safetensors.original'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state_dict['model.layers.9.self_attn.q_proj.g_idx'])\n",
    "# print(state_dict['model.layers.9.self_attn.k_proj.g_idx'])\n",
    "# print(state_dict['model.layers.9.self_attn.v_proj.g_idx'])\n",
    "# print(torch.equal(state_dict['model.layers.9.self_attn.q_proj.g_idx'], state_dict['model.layers.9.self_attn.k_proj.g_idx']) \n",
    "#       and torch.equal(state_dict['model.layers.9.self_attn.q_proj.g_idx'], state_dict['model.layers.9.self_attn.v_proj.g_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state_dict['model.layers.8.self_attn.q_proj.g_idx'])\n",
    "# print(state_dict['model.layers.8.self_attn.k_proj.g_idx'])\n",
    "# print(state_dict['model.layers.8.self_attn.v_proj.g_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state_dict['model.layers.8.self_attn.q_proj.qweight'])\n",
    "# print(state_dict['model.layers.8.self_attn.k_proj.qweight'])\n",
    "# print(state_dict['model.layers.8.self_attn.v_proj.qweight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state_dict['model.layers.8.self_attn.q_proj.scales'])\n",
    "# print(state_dict['model.layers.8.self_attn.k_proj.scales'])\n",
    "# print(state_dict['model.layers.8.self_attn.v_proj.scales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state_dict['model.layers.8.self_attn.q_proj.qzeros'])\n",
    "# print(state_dict['model.layers.8.self_attn.k_proj.qzeros'])\n",
    "# print(state_dict['model.layers.8.self_attn.v_proj.qzeros'])\n",
    "# torch.equal(state_dict['model.layers.8.self_attn.q_proj.qzeros'], state_dict['model.layers.8.self_attn.k_proj.qzeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6, 59, 18,  ..., 56, 12, 38], device='cuda:0', dtype=torch.int32)\n",
      "tensor([ 6, 59, 18,  ..., 56, 12, 38], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(state_dict['model.layers.9.mlp.up_proj.g_idx'])\n",
    "print(state_dict['model.layers.9.mlp.gate_proj.g_idx'])\n",
    "torch.equal(state_dict['model.layers.9.mlp.up_proj.g_idx'], state_dict['model.layers.9.mlp.gate_proj.g_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1504102265,  1989637763, -1450472794,  ...,  2007455285,\n",
       "         -1197963625,  2034726259],\n",
       "        [ 1150651017,  1719163430,  1806150534,  ..., -2070165900,\n",
       "          -861243228,  -912684695],\n",
       "        [ 1735931273, -1440978586, -2056701570,  ..., -1738033288,\n",
       "         -1735742587, -1717276089],\n",
       "        ...,\n",
       "        [ -390814618,  1986534773, -1720158039,  ..., -1445767350,\n",
       "         -1754683419,  1771063605],\n",
       "        [-1264268426, -2021487965,  2025306986,  ...,  1186240105,\n",
       "          1421317545, -1852012662],\n",
       "        [-1179142536,  -979978346,  1161476001,  ..., -1420248477,\n",
       "           968382584, -2033743465]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some examples\n",
    "state_dict['model.layers.9.mlp.up_proj.qweight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 59, 18,  ..., 56, 12, 38], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['model.layers.9.mlp.up_proj.g_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 28672])\n",
      "torch.Size([8192])\n",
      "torch.Size([64, 28672])\n",
      "torch.Size([64, 3584])\n"
     ]
    }
   ],
   "source": [
    "print(state_dict['model.layers.9.mlp.up_proj.qweight'].shape)\n",
    "print(state_dict['model.layers.9.mlp.up_proj.g_idx'].shape)\n",
    "print(state_dict['model.layers.9.mlp.up_proj.scales'].shape)\n",
    "print(state_dict['model.layers.9.mlp.up_proj.qzeros'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle columns of scales\n",
    "def shuffle_and_replace_scales(state_dict, scales_name, col_perm):\n",
    "    scales = state_dict[scales_name]\n",
    "    assert len(col_perm) == scales.shape[1]\n",
    "    \n",
    "    shuffled_scales = scales[:,col_perm]\n",
    "    state_dict[scales_name] = shuffled_scales\n",
    "\n",
    "def unpack_shuffle_repack_and_replace_qzeros(state_dict, bits, qzeros_name, col_perm):\n",
    "    qzeros = state_dict[qzeros_name]\n",
    "    mask = 2**bits - 1\n",
    "    pack_size = 32 // bits\n",
    "    assert len(col_perm) == qzeros.shape[1] * pack_size\n",
    "    \n",
    "    #unpack\n",
    "    unpacked_qzeros = torch.zeros((qzeros.shape[0], qzeros.shape[1]*pack_size), dtype=torch.int)\n",
    "    for i in range(pack_size):\n",
    "        unpacked_qzeros[:, i::pack_size] = (qzeros >> (i*bits)) & (mask) \n",
    "\n",
    "    # shuffle\n",
    "    shuffled_qzeros = unpacked_qzeros[:,col_perm]\n",
    "\n",
    "    # repack\n",
    "    packed_qzeros = torch.zeros_like(qzeros)\n",
    "    for i in range(pack_size):\n",
    "        packed_qzeros |= (shuffled_qzeros[:, i::pack_size] & mask) << (i*bits)     \n",
    "    \n",
    "    state_dict[qzeros_name] = packed_qzeros\n",
    "    \n",
    "def shuffle_and_replace_qweight(state_dict, bits, group_size, qweight_name, g_idx_name=None, next_g_idx_name=None, stable=False):\n",
    "    qweight = state_dict[qweight_name]\n",
    "\n",
    "    # unpack qweight\n",
    "    mask = 2**bits - 1\n",
    "    pack_size = 32 // bits\n",
    "    unpacked_qweight = torch.zeros((qweight.shape[0]*pack_size, qweight.shape[1]), dtype=torch.int)\n",
    "    for i in range(pack_size):\n",
    "        unpacked_qweight[i::pack_size] = (qweight >> (i*bits)) & (mask) \n",
    "\n",
    "    # reorder rows conditionally\n",
    "    if not (g_idx_name is None):\n",
    "        g_idx = state_dict[g_idx_name]\n",
    "        row_perm = torch.argsort(g_idx, stable=stable)\n",
    "        unpacked_qweight = unpacked_qweight[row_perm]\n",
    "    \n",
    "    # reorder columns conditionally\n",
    "    if not (next_g_idx_name is None):\n",
    "        next_g_idx = state_dict[next_g_idx_name]\n",
    "        col_perm = torch.argsort(next_g_idx, stable=stable)\n",
    "        unpacked_qweight = unpacked_qweight[:,col_perm]\n",
    "\n",
    "    # pack qweight\n",
    "    packed_qweight = torch.zeros_like(qweight)\n",
    "    for i in range(pack_size):\n",
    "        packed_qweight |= (unpacked_qweight[i::pack_size] & mask) << (i*bits) \n",
    "\n",
    "    # replace qweight with new reordered one in state_dict\n",
    "    print(f'replacing {qweight_name}')\n",
    "    state_dict[qweight_name] = packed_qweight\n",
    "    \n",
    "    if not (g_idx_name is None):\n",
    "        print(f'replacing {g_idx_name}')\n",
    "        state_dict[g_idx_name] = torch.arange(0, len(g_idx), dtype=torch.int) // group_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing model.layers.0.mlp.gate_proj.qweight\n",
      "replacing model.layers.0.mlp.up_proj.qweight\n",
      "replacing model.layers.1.mlp.gate_proj.qweight\n",
      "replacing model.layers.1.mlp.up_proj.qweight\n",
      "replacing model.layers.10.mlp.gate_proj.qweight\n",
      "replacing model.layers.10.mlp.up_proj.qweight\n",
      "replacing model.layers.11.mlp.gate_proj.qweight\n",
      "replacing model.layers.11.mlp.up_proj.qweight\n",
      "replacing model.layers.12.mlp.gate_proj.qweight\n",
      "replacing model.layers.12.mlp.up_proj.qweight\n",
      "replacing model.layers.13.mlp.gate_proj.qweight\n",
      "replacing model.layers.13.mlp.up_proj.qweight\n",
      "replacing model.layers.14.mlp.gate_proj.qweight\n",
      "replacing model.layers.14.mlp.up_proj.qweight\n",
      "replacing model.layers.15.mlp.gate_proj.qweight\n",
      "replacing model.layers.15.mlp.up_proj.qweight\n",
      "replacing model.layers.16.mlp.gate_proj.qweight\n",
      "replacing model.layers.16.mlp.up_proj.qweight\n",
      "replacing model.layers.17.mlp.gate_proj.qweight\n",
      "replacing model.layers.17.mlp.up_proj.qweight\n",
      "replacing model.layers.18.mlp.gate_proj.qweight\n",
      "replacing model.layers.18.mlp.up_proj.qweight\n",
      "replacing model.layers.19.mlp.gate_proj.qweight\n",
      "replacing model.layers.19.mlp.up_proj.qweight\n",
      "replacing model.layers.2.mlp.gate_proj.qweight\n",
      "replacing model.layers.2.mlp.up_proj.qweight\n",
      "replacing model.layers.20.mlp.gate_proj.qweight\n",
      "replacing model.layers.20.mlp.up_proj.qweight\n",
      "replacing model.layers.21.mlp.gate_proj.qweight\n",
      "replacing model.layers.21.mlp.up_proj.qweight\n",
      "replacing model.layers.22.mlp.gate_proj.qweight\n",
      "replacing model.layers.22.mlp.up_proj.qweight\n",
      "replacing model.layers.23.mlp.gate_proj.qweight\n",
      "replacing model.layers.23.mlp.up_proj.qweight\n",
      "replacing model.layers.24.mlp.gate_proj.qweight\n",
      "replacing model.layers.24.mlp.up_proj.qweight\n",
      "replacing model.layers.25.mlp.gate_proj.qweight\n",
      "replacing model.layers.25.mlp.up_proj.qweight\n",
      "replacing model.layers.26.mlp.gate_proj.qweight\n",
      "replacing model.layers.26.mlp.up_proj.qweight\n",
      "replacing model.layers.27.mlp.gate_proj.qweight\n",
      "replacing model.layers.27.mlp.up_proj.qweight\n",
      "replacing model.layers.28.mlp.gate_proj.qweight\n",
      "replacing model.layers.28.mlp.up_proj.qweight\n",
      "replacing model.layers.29.mlp.gate_proj.qweight\n",
      "replacing model.layers.29.mlp.up_proj.qweight\n",
      "replacing model.layers.3.mlp.gate_proj.qweight\n",
      "replacing model.layers.3.mlp.up_proj.qweight\n",
      "replacing model.layers.30.mlp.gate_proj.qweight\n",
      "replacing model.layers.30.mlp.up_proj.qweight\n",
      "replacing model.layers.31.mlp.gate_proj.qweight\n",
      "replacing model.layers.31.mlp.up_proj.qweight\n",
      "replacing model.layers.32.mlp.gate_proj.qweight\n",
      "replacing model.layers.32.mlp.up_proj.qweight\n",
      "replacing model.layers.33.mlp.gate_proj.qweight\n",
      "replacing model.layers.33.mlp.up_proj.qweight\n",
      "replacing model.layers.34.mlp.gate_proj.qweight\n",
      "replacing model.layers.34.mlp.up_proj.qweight\n",
      "replacing model.layers.35.mlp.gate_proj.qweight\n",
      "replacing model.layers.35.mlp.up_proj.qweight\n",
      "replacing model.layers.36.mlp.gate_proj.qweight\n",
      "replacing model.layers.36.mlp.up_proj.qweight\n",
      "replacing model.layers.37.mlp.gate_proj.qweight\n",
      "replacing model.layers.37.mlp.up_proj.qweight\n",
      "replacing model.layers.38.mlp.gate_proj.qweight\n",
      "replacing model.layers.38.mlp.up_proj.qweight\n",
      "replacing model.layers.39.mlp.gate_proj.qweight\n",
      "replacing model.layers.39.mlp.up_proj.qweight\n",
      "replacing model.layers.4.mlp.gate_proj.qweight\n",
      "replacing model.layers.4.mlp.up_proj.qweight\n",
      "replacing model.layers.40.mlp.gate_proj.qweight\n",
      "replacing model.layers.40.mlp.up_proj.qweight\n",
      "replacing model.layers.41.mlp.gate_proj.qweight\n",
      "replacing model.layers.41.mlp.up_proj.qweight\n",
      "replacing model.layers.42.mlp.gate_proj.qweight\n",
      "replacing model.layers.42.mlp.up_proj.qweight\n",
      "replacing model.layers.43.mlp.gate_proj.qweight\n",
      "replacing model.layers.43.mlp.up_proj.qweight\n",
      "replacing model.layers.44.mlp.gate_proj.qweight\n",
      "replacing model.layers.44.mlp.up_proj.qweight\n",
      "replacing model.layers.45.mlp.gate_proj.qweight\n",
      "replacing model.layers.45.mlp.up_proj.qweight\n",
      "replacing model.layers.46.mlp.gate_proj.qweight\n",
      "replacing model.layers.46.mlp.up_proj.qweight\n",
      "replacing model.layers.47.mlp.gate_proj.qweight\n",
      "replacing model.layers.47.mlp.up_proj.qweight\n",
      "replacing model.layers.48.mlp.gate_proj.qweight\n",
      "replacing model.layers.48.mlp.up_proj.qweight\n",
      "replacing model.layers.49.mlp.gate_proj.qweight\n",
      "replacing model.layers.49.mlp.up_proj.qweight\n",
      "replacing model.layers.5.mlp.gate_proj.qweight\n",
      "replacing model.layers.5.mlp.up_proj.qweight\n",
      "replacing model.layers.50.mlp.gate_proj.qweight\n",
      "replacing model.layers.50.mlp.up_proj.qweight\n",
      "replacing model.layers.51.mlp.gate_proj.qweight\n",
      "replacing model.layers.51.mlp.up_proj.qweight\n",
      "replacing model.layers.52.mlp.gate_proj.qweight\n",
      "replacing model.layers.52.mlp.up_proj.qweight\n",
      "replacing model.layers.53.mlp.gate_proj.qweight\n",
      "replacing model.layers.53.mlp.up_proj.qweight\n",
      "replacing model.layers.54.mlp.gate_proj.qweight\n",
      "replacing model.layers.54.mlp.up_proj.qweight\n",
      "replacing model.layers.55.mlp.gate_proj.qweight\n",
      "replacing model.layers.55.mlp.up_proj.qweight\n",
      "replacing model.layers.56.mlp.gate_proj.qweight\n",
      "replacing model.layers.56.mlp.up_proj.qweight\n",
      "replacing model.layers.57.mlp.gate_proj.qweight\n",
      "replacing model.layers.57.mlp.up_proj.qweight\n",
      "replacing model.layers.58.mlp.gate_proj.qweight\n",
      "replacing model.layers.58.mlp.up_proj.qweight\n",
      "replacing model.layers.59.mlp.gate_proj.qweight\n",
      "replacing model.layers.59.mlp.up_proj.qweight\n",
      "replacing model.layers.6.mlp.gate_proj.qweight\n",
      "replacing model.layers.6.mlp.up_proj.qweight\n",
      "replacing model.layers.60.mlp.gate_proj.qweight\n",
      "replacing model.layers.60.mlp.up_proj.qweight\n",
      "replacing model.layers.61.mlp.gate_proj.qweight\n",
      "replacing model.layers.61.mlp.up_proj.qweight\n",
      "replacing model.layers.62.mlp.gate_proj.qweight\n",
      "replacing model.layers.62.mlp.up_proj.qweight\n",
      "replacing model.layers.63.mlp.gate_proj.qweight\n",
      "replacing model.layers.63.mlp.up_proj.qweight\n",
      "replacing model.layers.64.mlp.gate_proj.qweight\n",
      "replacing model.layers.64.mlp.up_proj.qweight\n",
      "replacing model.layers.65.mlp.gate_proj.qweight\n",
      "replacing model.layers.65.mlp.up_proj.qweight\n",
      "replacing model.layers.66.mlp.gate_proj.qweight\n",
      "replacing model.layers.66.mlp.up_proj.qweight\n",
      "replacing model.layers.67.mlp.gate_proj.qweight\n",
      "replacing model.layers.67.mlp.up_proj.qweight\n",
      "replacing model.layers.68.mlp.gate_proj.qweight\n",
      "replacing model.layers.68.mlp.up_proj.qweight\n",
      "replacing model.layers.69.mlp.gate_proj.qweight\n",
      "replacing model.layers.69.mlp.up_proj.qweight\n",
      "replacing model.layers.7.mlp.gate_proj.qweight\n",
      "replacing model.layers.7.mlp.up_proj.qweight\n",
      "replacing model.layers.70.mlp.gate_proj.qweight\n",
      "replacing model.layers.70.mlp.up_proj.qweight\n",
      "replacing model.layers.71.mlp.gate_proj.qweight\n",
      "replacing model.layers.71.mlp.up_proj.qweight\n",
      "replacing model.layers.72.mlp.gate_proj.qweight\n",
      "replacing model.layers.72.mlp.up_proj.qweight\n",
      "replacing model.layers.73.mlp.gate_proj.qweight\n",
      "replacing model.layers.73.mlp.up_proj.qweight\n",
      "replacing model.layers.74.mlp.gate_proj.qweight\n",
      "replacing model.layers.74.mlp.up_proj.qweight\n",
      "replacing model.layers.75.mlp.gate_proj.qweight\n",
      "replacing model.layers.75.mlp.up_proj.qweight\n",
      "replacing model.layers.76.mlp.gate_proj.qweight\n",
      "replacing model.layers.76.mlp.up_proj.qweight\n",
      "replacing model.layers.77.mlp.gate_proj.qweight\n",
      "replacing model.layers.77.mlp.up_proj.qweight\n",
      "replacing model.layers.78.mlp.gate_proj.qweight\n",
      "replacing model.layers.78.mlp.up_proj.qweight\n",
      "replacing model.layers.79.mlp.gate_proj.qweight\n",
      "replacing model.layers.79.mlp.up_proj.qweight\n",
      "replacing model.layers.8.mlp.gate_proj.qweight\n",
      "replacing model.layers.8.mlp.up_proj.qweight\n",
      "replacing model.layers.9.mlp.gate_proj.qweight\n",
      "replacing model.layers.9.mlp.up_proj.qweight\n",
      "replacing model.layers.0.mlp.down_proj.qweight\n",
      "replacing model.layers.0.mlp.down_proj.g_idx\n",
      "replacing model.layers.1.mlp.down_proj.qweight\n",
      "replacing model.layers.1.mlp.down_proj.g_idx\n",
      "replacing model.layers.10.mlp.down_proj.qweight\n",
      "replacing model.layers.10.mlp.down_proj.g_idx\n",
      "replacing model.layers.11.mlp.down_proj.qweight\n",
      "replacing model.layers.11.mlp.down_proj.g_idx\n",
      "replacing model.layers.12.mlp.down_proj.qweight\n",
      "replacing model.layers.12.mlp.down_proj.g_idx\n",
      "replacing model.layers.13.mlp.down_proj.qweight\n",
      "replacing model.layers.13.mlp.down_proj.g_idx\n",
      "replacing model.layers.14.mlp.down_proj.qweight\n",
      "replacing model.layers.14.mlp.down_proj.g_idx\n",
      "replacing model.layers.15.mlp.down_proj.qweight\n",
      "replacing model.layers.15.mlp.down_proj.g_idx\n",
      "replacing model.layers.16.mlp.down_proj.qweight\n",
      "replacing model.layers.16.mlp.down_proj.g_idx\n",
      "replacing model.layers.17.mlp.down_proj.qweight\n",
      "replacing model.layers.17.mlp.down_proj.g_idx\n",
      "replacing model.layers.18.mlp.down_proj.qweight\n",
      "replacing model.layers.18.mlp.down_proj.g_idx\n",
      "replacing model.layers.19.mlp.down_proj.qweight\n",
      "replacing model.layers.19.mlp.down_proj.g_idx\n",
      "replacing model.layers.2.mlp.down_proj.qweight\n",
      "replacing model.layers.2.mlp.down_proj.g_idx\n",
      "replacing model.layers.20.mlp.down_proj.qweight\n",
      "replacing model.layers.20.mlp.down_proj.g_idx\n",
      "replacing model.layers.21.mlp.down_proj.qweight\n",
      "replacing model.layers.21.mlp.down_proj.g_idx\n",
      "replacing model.layers.22.mlp.down_proj.qweight\n",
      "replacing model.layers.22.mlp.down_proj.g_idx\n",
      "replacing model.layers.23.mlp.down_proj.qweight\n",
      "replacing model.layers.23.mlp.down_proj.g_idx\n",
      "replacing model.layers.24.mlp.down_proj.qweight\n",
      "replacing model.layers.24.mlp.down_proj.g_idx\n",
      "replacing model.layers.25.mlp.down_proj.qweight\n",
      "replacing model.layers.25.mlp.down_proj.g_idx\n",
      "replacing model.layers.26.mlp.down_proj.qweight\n",
      "replacing model.layers.26.mlp.down_proj.g_idx\n",
      "replacing model.layers.27.mlp.down_proj.qweight\n",
      "replacing model.layers.27.mlp.down_proj.g_idx\n",
      "replacing model.layers.28.mlp.down_proj.qweight\n",
      "replacing model.layers.28.mlp.down_proj.g_idx\n",
      "replacing model.layers.29.mlp.down_proj.qweight\n",
      "replacing model.layers.29.mlp.down_proj.g_idx\n",
      "replacing model.layers.3.mlp.down_proj.qweight\n",
      "replacing model.layers.3.mlp.down_proj.g_idx\n",
      "replacing model.layers.30.mlp.down_proj.qweight\n",
      "replacing model.layers.30.mlp.down_proj.g_idx\n",
      "replacing model.layers.31.mlp.down_proj.qweight\n",
      "replacing model.layers.31.mlp.down_proj.g_idx\n",
      "replacing model.layers.32.mlp.down_proj.qweight\n",
      "replacing model.layers.32.mlp.down_proj.g_idx\n",
      "replacing model.layers.33.mlp.down_proj.qweight\n",
      "replacing model.layers.33.mlp.down_proj.g_idx\n",
      "replacing model.layers.34.mlp.down_proj.qweight\n",
      "replacing model.layers.34.mlp.down_proj.g_idx\n",
      "replacing model.layers.35.mlp.down_proj.qweight\n",
      "replacing model.layers.35.mlp.down_proj.g_idx\n",
      "replacing model.layers.36.mlp.down_proj.qweight\n",
      "replacing model.layers.36.mlp.down_proj.g_idx\n",
      "replacing model.layers.37.mlp.down_proj.qweight\n",
      "replacing model.layers.37.mlp.down_proj.g_idx\n",
      "replacing model.layers.38.mlp.down_proj.qweight\n",
      "replacing model.layers.38.mlp.down_proj.g_idx\n",
      "replacing model.layers.39.mlp.down_proj.qweight\n",
      "replacing model.layers.39.mlp.down_proj.g_idx\n",
      "replacing model.layers.4.mlp.down_proj.qweight\n",
      "replacing model.layers.4.mlp.down_proj.g_idx\n",
      "replacing model.layers.40.mlp.down_proj.qweight\n",
      "replacing model.layers.40.mlp.down_proj.g_idx\n",
      "replacing model.layers.41.mlp.down_proj.qweight\n",
      "replacing model.layers.41.mlp.down_proj.g_idx\n",
      "replacing model.layers.42.mlp.down_proj.qweight\n",
      "replacing model.layers.42.mlp.down_proj.g_idx\n",
      "replacing model.layers.43.mlp.down_proj.qweight\n",
      "replacing model.layers.43.mlp.down_proj.g_idx\n",
      "replacing model.layers.44.mlp.down_proj.qweight\n",
      "replacing model.layers.44.mlp.down_proj.g_idx\n",
      "replacing model.layers.45.mlp.down_proj.qweight\n",
      "replacing model.layers.45.mlp.down_proj.g_idx\n",
      "replacing model.layers.46.mlp.down_proj.qweight\n",
      "replacing model.layers.46.mlp.down_proj.g_idx\n",
      "replacing model.layers.47.mlp.down_proj.qweight\n",
      "replacing model.layers.47.mlp.down_proj.g_idx\n",
      "replacing model.layers.48.mlp.down_proj.qweight\n",
      "replacing model.layers.48.mlp.down_proj.g_idx\n",
      "replacing model.layers.49.mlp.down_proj.qweight\n",
      "replacing model.layers.49.mlp.down_proj.g_idx\n",
      "replacing model.layers.5.mlp.down_proj.qweight\n",
      "replacing model.layers.5.mlp.down_proj.g_idx\n",
      "replacing model.layers.50.mlp.down_proj.qweight\n",
      "replacing model.layers.50.mlp.down_proj.g_idx\n",
      "replacing model.layers.51.mlp.down_proj.qweight\n",
      "replacing model.layers.51.mlp.down_proj.g_idx\n",
      "replacing model.layers.52.mlp.down_proj.qweight\n",
      "replacing model.layers.52.mlp.down_proj.g_idx\n",
      "replacing model.layers.53.mlp.down_proj.qweight\n",
      "replacing model.layers.53.mlp.down_proj.g_idx\n",
      "replacing model.layers.54.mlp.down_proj.qweight\n",
      "replacing model.layers.54.mlp.down_proj.g_idx\n",
      "replacing model.layers.55.mlp.down_proj.qweight\n",
      "replacing model.layers.55.mlp.down_proj.g_idx\n",
      "replacing model.layers.56.mlp.down_proj.qweight\n",
      "replacing model.layers.56.mlp.down_proj.g_idx\n",
      "replacing model.layers.57.mlp.down_proj.qweight\n",
      "replacing model.layers.57.mlp.down_proj.g_idx\n",
      "replacing model.layers.58.mlp.down_proj.qweight\n",
      "replacing model.layers.58.mlp.down_proj.g_idx\n",
      "replacing model.layers.59.mlp.down_proj.qweight\n",
      "replacing model.layers.59.mlp.down_proj.g_idx\n",
      "replacing model.layers.6.mlp.down_proj.qweight\n",
      "replacing model.layers.6.mlp.down_proj.g_idx\n",
      "replacing model.layers.60.mlp.down_proj.qweight\n",
      "replacing model.layers.60.mlp.down_proj.g_idx\n",
      "replacing model.layers.61.mlp.down_proj.qweight\n",
      "replacing model.layers.61.mlp.down_proj.g_idx\n",
      "replacing model.layers.62.mlp.down_proj.qweight\n",
      "replacing model.layers.62.mlp.down_proj.g_idx\n",
      "replacing model.layers.63.mlp.down_proj.qweight\n",
      "replacing model.layers.63.mlp.down_proj.g_idx\n",
      "replacing model.layers.64.mlp.down_proj.qweight\n",
      "replacing model.layers.64.mlp.down_proj.g_idx\n",
      "replacing model.layers.65.mlp.down_proj.qweight\n",
      "replacing model.layers.65.mlp.down_proj.g_idx\n",
      "replacing model.layers.66.mlp.down_proj.qweight\n",
      "replacing model.layers.66.mlp.down_proj.g_idx\n",
      "replacing model.layers.67.mlp.down_proj.qweight\n",
      "replacing model.layers.67.mlp.down_proj.g_idx\n",
      "replacing model.layers.68.mlp.down_proj.qweight\n",
      "replacing model.layers.68.mlp.down_proj.g_idx\n",
      "replacing model.layers.69.mlp.down_proj.qweight\n",
      "replacing model.layers.69.mlp.down_proj.g_idx\n",
      "replacing model.layers.7.mlp.down_proj.qweight\n",
      "replacing model.layers.7.mlp.down_proj.g_idx\n",
      "replacing model.layers.70.mlp.down_proj.qweight\n",
      "replacing model.layers.70.mlp.down_proj.g_idx\n",
      "replacing model.layers.71.mlp.down_proj.qweight\n",
      "replacing model.layers.71.mlp.down_proj.g_idx\n",
      "replacing model.layers.72.mlp.down_proj.qweight\n",
      "replacing model.layers.72.mlp.down_proj.g_idx\n",
      "replacing model.layers.73.mlp.down_proj.qweight\n",
      "replacing model.layers.73.mlp.down_proj.g_idx\n",
      "replacing model.layers.74.mlp.down_proj.qweight\n",
      "replacing model.layers.74.mlp.down_proj.g_idx\n",
      "replacing model.layers.75.mlp.down_proj.qweight\n",
      "replacing model.layers.75.mlp.down_proj.g_idx\n",
      "replacing model.layers.76.mlp.down_proj.qweight\n",
      "replacing model.layers.76.mlp.down_proj.g_idx\n",
      "replacing model.layers.77.mlp.down_proj.qweight\n",
      "replacing model.layers.77.mlp.down_proj.g_idx\n",
      "replacing model.layers.78.mlp.down_proj.qweight\n",
      "replacing model.layers.78.mlp.down_proj.g_idx\n",
      "replacing model.layers.79.mlp.down_proj.qweight\n",
      "replacing model.layers.79.mlp.down_proj.g_idx\n",
      "replacing model.layers.8.mlp.down_proj.qweight\n",
      "replacing model.layers.8.mlp.down_proj.g_idx\n",
      "replacing model.layers.9.mlp.down_proj.qweight\n",
      "replacing model.layers.9.mlp.down_proj.g_idx\n"
     ]
    }
   ],
   "source": [
    "# Process column parallel layers first\n",
    "# Reorder their columns only \n",
    "for name in state_dict.keys():\n",
    "    if 'mlp.gate_proj.qweight' in name: # col-parallel layer\n",
    "        qweight_name = name\n",
    "        next_g_idx_name = name.replace('gate_proj.qweight', 'down_proj.g_idx')\n",
    "        scales_name = name.replace('qweight', 'scales')\n",
    "        qzeros_name = name.replace('qweight', 'qzeros')\n",
    "        next_g_idx = state_dict[next_g_idx_name]\n",
    "        perm = torch.argsort(next_g_idx)\n",
    "        \n",
    "        shuffle_and_replace_scales(state_dict, scales_name, perm)\n",
    "        unpack_shuffle_repack_and_replace_qzeros(state_dict, bits, qzeros_name, perm)\n",
    "        shuffle_and_replace_qweight(state_dict, bits, group_size, qweight_name, next_g_idx_name=next_g_idx_name)\n",
    "    elif 'mlp.up_proj.qweight' in name: # col-parallel layer\n",
    "        qweight_name = name\n",
    "        next_g_idx_name = name.replace('up_proj.qweight', 'down_proj.g_idx')\n",
    "        scales_name = name.replace('qweight', 'scales')\n",
    "        qzeros_name = name.replace('qweight', 'qzeros')\n",
    "        next_g_idx = state_dict[next_g_idx_name]\n",
    "        perm = torch.argsort(next_g_idx)\n",
    "        \n",
    "        shuffle_and_replace_scales(state_dict, scales_name, perm)\n",
    "        unpack_shuffle_repack_and_replace_qzeros(state_dict, bits, qzeros_name, perm)\n",
    "        shuffle_and_replace_qweight(state_dict, bits, group_size, qweight_name, next_g_idx_name=next_g_idx_name)\n",
    "\n",
    "# Process row parallel layers after all column parallel layers because of dependency on g_idx\n",
    "for name in state_dict.keys():        \n",
    "    if 'mlp.down_proj.qweight' in name: # row-parallel layer\n",
    "        qweight_name = name\n",
    "        g_idx_name = name.replace('qweight', 'g_idx')\n",
    "        shuffle_and_replace_qweight(state_dict, bits, group_size, qweight_name, g_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(state_dict, os.path.join(checkpoint_path, 'model.safetensors.reordered'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11_triton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
