{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee60d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d529c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.models.llama import LLaMAConfig, LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1504b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = LLaMAConfig(32000, 4096, 1e-6, 32, 0, 32)\n",
    "model = LLaMA(modelc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4558b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(\"../../../llama_7b_ckp.pth\")['model_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce3e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = list(d.keys())\n",
    "for key in keylist:\n",
    "    if \"dec_process\" in key:\n",
    "        value = d.pop(key)\n",
    "        fields = key.split(\".\")\n",
    "        fields[0] = \"layers\"\n",
    "        d[\".\".join(fields)] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83097771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['rope.freqs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(d, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57035028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.utils.generation import generate\n",
    "from transformers import AutoTokenizer\n",
    "t = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "vinv = {v:k for k,v in t.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5616faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?']\n"
     ]
    }
   ],
   "source": [
    "inp = t(\"Hello! How are you today?\")[\"input_ids\"]\n",
    "print([vinv[x] for x in inp])\n",
    "inp = torch.IntTensor(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "762260eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Hello! How are you today? I hope you are doing well. I am doing well. I am happy to be here with you'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle = generate(model, inp, 20, 20, do_sample=False, use_cache=True)\n",
    "t.decode(oracle.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc6fd98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Speculator(n_heads=3)\n",
    "test.load_state_dict(torch.load(\"../../../specu_greedy.pth\", map_location=\"cpu\")[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08810d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844107776"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in test.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aecabfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk@1: ['▁I', '<0x0A>', '!', '▁We', '▁my']\n",
      "Topk@2: ['▁hope', '▁you', 'I', \"'\", '’']\n",
      "Topk@3: ['▁you', 'm', '▁I', '▁is', '▁a']\n",
      "Speculation: ['?', '▁I', '▁hope', '▁you']\n",
      "Verification: ['▁I', '▁hope', '▁you', '▁are'] 3\n",
      "Acc@5: 3\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are']\n",
      "\n",
      "Topk@1: ['re', '▁having', '▁doing', '▁a', '▁had']\n",
      "Topk@2: ['▁having', '▁well', '▁great', '▁doing', '.']\n",
      "Topk@3: ['▁great', '.', '▁well', '▁a', 'ying']\n",
      "Speculation: ['▁are', 're', '▁having', '▁great']\n",
      "Verification: ['▁doing', 'ally', '▁a', '▁day'] 0\n",
      "Acc@5: 3\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing']\n",
      "\n",
      "Topk@1: ['▁well', '.', '▁a', '▁good', 'ying']\n",
      "Topk@2: ['▁great', '.', '▁and', '▁I', '▁good']\n",
      "Topk@3: ['▁I', '▁day', '.', '<0x0A>', '▁great']\n",
      "Speculation: ['▁doing', '▁well', '▁great', '▁I']\n",
      "Verification: ['▁well', '.', '.', '▁am'] 1\n",
      "Acc@5: 3\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.']\n",
      "\n",
      "Topk@1: ['▁I', '<0x0A>', '▁and', '▁that', '▁enjo']\n",
      "Topk@2: ['I', '▁you', '▁I', '▁are', '▁hope']\n",
      "Topk@3: ['▁I', '▁you', '▁are', '▁a', '▁been']\n",
      "Speculation: ['.', '▁I', 'I', '▁I']\n",
      "Verification: ['▁I', '▁am', '▁am', '▁am'] 1\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am']\n",
      "\n",
      "Topk@1: ['▁you', 'm', '▁a', '▁been', '▁so']\n",
      "Topk@2: ['▁been', '▁to', '▁are', '▁you', '▁excited']\n",
      "Topk@3: ['▁you', '▁to', '▁a', '▁with', '▁I']\n",
      "Speculation: ['▁am', '▁you', '▁been', '▁you']\n",
      "Verification: ['▁doing', '▁are', '▁busy', '▁know'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing']\n",
      "\n",
      "Topk@1: ['▁to', '▁a', '▁well', '▁doing', '▁excited']\n",
      "Topk@2: ['▁to', '▁I', '.', '▁and', '▁well']\n",
      "Topk@3: ['▁I', '.', '▁you', '▁today', '▁a']\n",
      "Speculation: ['▁doing', '▁to', '▁to', '▁I']\n",
      "Verification: ['▁well', '▁be', '▁be', '▁am'] 0\n",
      "Acc@5: 3\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well']\n",
      "\n",
      "Topk@1: ['.', '▁well', '▁good', '▁today', '▁fine']\n",
      "Topk@2: ['▁I', '.', '<0x0A>', '▁today', '▁thank']\n",
      "Topk@3: ['▁I', '▁am', '.', 'I', '▁have']\n",
      "Speculation: ['▁well', '.', '▁I', '▁I']\n",
      "Verification: ['.', '▁I', '▁am', '▁am'] 2\n",
      "Acc@5: 3\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am']\n",
      "\n",
      "Topk@1: ['▁been', '▁a', '▁to', 'm', '▁so']\n",
      "Topk@2: ['▁to', '▁been', '▁a', '▁busy', '▁happy']\n",
      "Topk@3: ['▁to', '▁a', '▁with', '▁my', '▁and']\n",
      "Speculation: ['▁am', '▁been', '▁to', '▁to']\n",
      "Verification: ['▁happy', '▁busy', '▁the', '▁the'] 0\n",
      "Acc@5: 0\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy']\n",
      "\n",
      "Topk@1: ['▁to', '▁a', '▁happy', '▁with', '▁excited']\n",
      "Topk@2: ['▁to', '▁the', '.', '▁my', '▁a']\n",
      "Topk@3: ['▁I', '.', '▁the', '▁my', '▁a']\n",
      "Speculation: ['▁happy', '▁to', '▁to', '▁I']\n",
      "Verification: ['▁to', '▁be', '▁be', '▁am'] 1\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be']\n",
      "\n",
      "Topk@1: ['▁here', '▁you', '▁that', '▁back', '▁with']\n",
      "Topk@2: ['▁here', '▁you', '▁I', '▁today', '▁that']\n",
      "Topk@3: ['▁I', '.', '▁my', '▁today', '▁you']\n",
      "Speculation: ['▁be', '▁here', '▁here', '▁I']\n",
      "Verification: ['▁here', '▁with', '▁with', '▁am'] 1\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with']\n",
      "\n",
      "Topk@1: ['▁you', '▁share', '▁I', '.', '▁sharing']\n",
      "Topk@2: ['▁you', '.', '▁blog', '▁I', '▁am']\n",
      "Topk@3: ['▁you', '▁I', '.', '▁blog', '▁is']\n",
      "Speculation: ['▁with', '▁you', '▁you', '▁you']\n",
      "Verification: ['▁you', '.', '▁today', '.'] 1\n",
      "Acc@5: 1\n",
      "Updated output: ['<s>', '▁Hello', '!', '▁How', '▁are', '▁you', '▁today', '?', '▁I', '▁hope', '▁you', '▁are', '▁doing', '▁well', '.', '▁I', '▁am', '▁doing', '▁well', '.', '▁I', '▁am', '▁happy', '▁to', '▁be', '▁here', '▁with', '▁you', '.']\n",
      "\n",
      "\n",
      "Steps: 11\n"
     ]
    }
   ],
   "source": [
    "out, steps = speculative_generate(model, inp, test, 20, 20)\n",
    "print()\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0eeebbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.modules.layernorm import LayerNormParameterized\n",
    "\n",
    "class Speculator(nn.Module):\n",
    "    def __init__(self, emb_dim=4096, vocab_size=32000, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.nheads = n_heads\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vsize = vocab_size\n",
    "        self.w_in = nn.Parameter(torch.empty(emb_dim, int((emb_dim * 2.6875 * 2) // 256) * 256 * 2))  # d 2z\n",
    "        self.a = nn.GELU()\n",
    "        self.w_out = nn.Parameter(torch.empty(int((emb_dim * 2.6875 * 2) // 256) * 256, emb_dim * n_heads))  # z hd\n",
    "        self.ln = LayerNormParameterized(emb_dim, elementwise_shift=False, elementwise_scale=True)\n",
    "        self.head = nn.Parameter(torch.empty(n_heads, emb_dim, vocab_size))  # h d v\n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        nn.init.trunc_normal_(self.w_in, 0, (1 / 2.6875) ** (1 / 6) / self.emb_dim**0.5)\n",
    "        nn.init.trunc_normal_(self.w_out, 0, (1 / 2.6875) ** (1 / 6) / self.emb_dim**0.5)\n",
    "        nn.init.trunc_normal_(self.head, 0, 1 / self.emb_dim**0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: b n d\n",
    "        z, g = x.matmul(self.w_in).chunk(2, dim=2)\n",
    "        z = z * self.a(g)\n",
    "        z = z.matmul(self.w_out).view(x.size(0), x.size(1), self.nheads, self.emb_dim)  # b n h d\n",
    "        z = z + x.unsqueeze(2)\n",
    "        z = self.ln(z)\n",
    "        z = torch.einsum(\"bnhd,hdv->bnhv\", z, self.head)\n",
    "        return z # b n h v\n",
    "    \n",
    "def decode_obo(x):\n",
    "    return [vinv[z] for z in x.squeeze().tolist()]\n",
    "    \n",
    "def speculative_generate(\n",
    "    model: Union[Callable, torch.nn.Module],\n",
    "    input_ids: torch.LongTensor,\n",
    "    smallmodel: torch.nn.Module,\n",
    "    max_seq_len: int = 2048,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 10,\n",
    "    num_beams: int = 1,\n",
    "):\n",
    "    do_sample = False\n",
    "    batched = False\n",
    "    if num_beams != 1:\n",
    "        raise NotImplementedError(\"generate() does yet not support beam search\")\n",
    "    if type(input_ids) == torch.Tensor:\n",
    "        if input_ids.dim() != 1:\n",
    "            batched = True\n",
    "    else:\n",
    "        raise RuntimeError(\"generate() requires a tensor of token ids as the prefix\")\n",
    "\n",
    "    if not batched:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    result = input_ids\n",
    "    next_input = input_ids\n",
    "    kwargs = dict()\n",
    "    kwargs[\"past_key_value_states\"] = None\n",
    "    kwargs[\"use_cache\"] = True\n",
    "\n",
    "    output = model(input_ids[:,:-1], include_embeds=True, **kwargs)\n",
    "    _, past_key_value_states, embeds = output\n",
    "    kwargs[\"past_key_value_states\"] = past_key_value_states\n",
    "    next_input = next_input[:,-1:]\n",
    "    \n",
    "    n_gen = 0\n",
    "    n_steps = 0\n",
    "    n_kv_s = past_key_value_states\n",
    "    while n_gen < max_new_tokens:\n",
    "        n_steps += 1\n",
    "        input_ids = next_input[:, -max_seq_len:]\n",
    "        \n",
    "        adds = smallmodel(embeds[:,-1].unsqueeze(1))\n",
    "        n_adds = smallmodel.nheads\n",
    "        \n",
    "        topk = adds.topk(5, dim=3)[1]\n",
    "        for i in range(n_adds):\n",
    "            print(f\"Topk@{i+1}:\", decode_obo(topk[0,0,i]))\n",
    "        adds = adds.argmax(3).squeeze(1) # b h\n",
    "        input_ids = torch.cat([input_ids, adds], dim=-1)\n",
    "        mask = torch.ones(input_ids.size(1),input_ids.size(1)+n_kv_s[0][0].size(2))\n",
    "        mask = mask.tril(diagonal=mask.size(1)-mask.size(0))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0).log()\n",
    "        print(\"Speculation:\", decode_obo(input_ids))\n",
    "        output = model.forward(input_ids, include_embeds=True, mask=mask, **kwargs)\n",
    "        logits, past_key_value_states, embeds = output\n",
    "        logits = logits[:, -n_adds-1:, :]\n",
    "\n",
    "        if do_sample:\n",
    "            # get logits from last value in sequence and scale\n",
    "#             logits = logits / temperature\n",
    "#             if top_k:\n",
    "#                 v, _ = torch.topk(logits, top_k)\n",
    "#                 logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "#             probs = F.softmax(logits, dim=-1)\n",
    "#             next_val = torch.multinomial(probs, num_samples=1)\n",
    "            assert False\n",
    "        else:\n",
    "            next_vals = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Check correctness of smallmodel predictions\n",
    "        n_correct = 0\n",
    "        while n_correct < n_adds and next_vals[0,n_correct] == input_ids[0,-n_adds+n_correct]:\n",
    "            n_correct += 1\n",
    "        print(\"Verification:\", decode_obo(next_vals), n_correct)\n",
    "        \n",
    "        k_correct = 0\n",
    "        while (\n",
    "            result.size(1)+k_correct < len(oracle) and \n",
    "            k_correct < n_adds and\n",
    "            oracle[result.size(1)+k_correct] in topk[0,0,k_correct]\n",
    "        ):\n",
    "            k_correct += 1\n",
    "        print(\"Acc@5:\", k_correct)\n",
    "        \n",
    "        # Toss any wrong smallmodel outputs\n",
    "        next_vals = next_vals[:,:n_correct+1]\n",
    "        n_gen += n_correct+1\n",
    "        embeds = embeds[:,:n_correct+1]\n",
    "            \n",
    "        n_wrong = n_adds - n_correct\n",
    "        # kv updates are required for torch.compile with\n",
    "        # mode='reduce-overhead'\n",
    "        n_kv_s = []\n",
    "        for layer_idx in range(len(past_key_value_states)):\n",
    "            n_kv_s.append([])\n",
    "            for tensor_idx in range(len(past_key_value_states[layer_idx])):\n",
    "                base = past_key_value_states[layer_idx][tensor_idx]\n",
    "                if n_wrong > 0:\n",
    "                    base = base[:,:,:-n_wrong]\n",
    "                n_kv_s[layer_idx].append(\n",
    "                    base.clone(memory_format=torch.contiguous_format).detach()\n",
    "                )\n",
    "                # torch._dynamo.mark_dynamic(n_kv_s[layer_idx][tensor_idx], 2)\n",
    "        kwargs[\"past_key_value_states\"] = n_kv_s\n",
    "\n",
    "        result = torch.cat((result, next_vals), dim=-1)\n",
    "        print(\"Updated output:\", decode_obo(result))\n",
    "        print()\n",
    "\n",
    "        next_input = next_vals[:,-1].unsqueeze(-1)\n",
    "\n",
    "    if not batched:\n",
    "        result = result[0]\n",
    "    return result, n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfac5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ff10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbcdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ca86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b8b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d39218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53790f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4165d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculation: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7, 52,  6, 24,  6]])\n",
      "Verification: tensor([[ 52,   6,  24,   6, 108]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108]])\n",
      "\n",
      "Speculation: tensor([[108,   0,   0,   0,   0]])\n",
      "Verification: tensor([[121,  22,  22,  22, 118]]) 0\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121]])\n",
      "\n",
      "Speculation: tensor([[121,  24,  35,  73,  60]])\n",
      "Verification: tensor([[24, 35, 73, 60, 75]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75]])\n",
      "\n",
      "Speculation: tensor([[75, 28, 24, 60,  0]])\n",
      "Verification: tensor([[ 28,  24,  60, 112,  35]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112]])\n",
      "\n",
      "Speculation: tensor([[112, 118,  73,  92,   0]])\n",
      "Verification: tensor([[118,  73,  92, 124,  69]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124]])\n",
      "\n",
      "Speculation: tensor([[124,  96]])\n",
      "Verification: tensor([[96,  6]]) 1\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124,  96,\n",
      "           6]])\n",
      "\n",
      "0.11324882507324219\n",
      "Steps: 6\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "out, steps = speculative_generate(model, inp, 20, 20)\n",
    "print(time.time()-start)\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5601e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(out)):\n",
    "    assert out[0,i]==oracle[0,i], i\n",
    "print(\"Exact match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbox_oracle(inp):\n",
    "    out = oracle[:,inp.size(1):inp.size(1)+4].clone()\n",
    "    nzero = torch.randint(5,(1,))\n",
    "    if nzero > 0:\n",
    "        out[:,-nzero:] = 0\n",
    "    return out, min(oracle.size(1), inp.size(1)+4) - inp.size(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
