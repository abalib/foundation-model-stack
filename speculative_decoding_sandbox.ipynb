{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee60d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d529c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fms.models.llama import LLaMAConfig, LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1504b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = LLaMAConfig(128, 512, 1e-6, 16, 0, 8)\n",
    "model = LLaMA(modelc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e8d518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23375678062438965\n",
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "         24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124,  96])\n"
     ]
    }
   ],
   "source": [
    "from fms.utils.generation import generate\n",
    "\n",
    "inp = torch.arange(8).unsqueeze(0)\n",
    "\n",
    "start = time.time()\n",
    "oracle = generate(model, inp, 20, 20, do_sample=False, use_cache=True)\n",
    "print(time.time()-start)\n",
    "\n",
    "print(oracle.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4165d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculation: tensor([[ 0,  1,  2,  3,  4,  5,  6,  7, 52,  6, 24,  6]])\n",
      "Verification: tensor([[ 52,   6,  24,   6, 108]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108]])\n",
      "\n",
      "Speculation: tensor([[108,   0,   0,   0,   0]])\n",
      "Verification: tensor([[121,  22,  22,  22, 118]]) 0\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121]])\n",
      "\n",
      "Speculation: tensor([[121,  24,  35,  73,  60]])\n",
      "Verification: tensor([[24, 35, 73, 60, 75]]) 4\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75]])\n",
      "\n",
      "Speculation: tensor([[75, 28, 24, 60,  0]])\n",
      "Verification: tensor([[ 28,  24,  60, 112,  35]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112]])\n",
      "\n",
      "Speculation: tensor([[112, 118,  73,  92,   0]])\n",
      "Verification: tensor([[118,  73,  92, 124,  69]]) 3\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124]])\n",
      "\n",
      "Speculation: tensor([[124,  96]])\n",
      "Verification: tensor([[96,  6]]) 1\n",
      "Updated output: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,  52,   6,  24,   6, 108, 121,\n",
      "          24,  35,  73,  60,  75,  28,  24,  60, 112, 118,  73,  92, 124,  96,\n",
      "           6]])\n",
      "\n",
      "0.11324882507324219\n",
      "Steps: 6\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "out, steps = speculative_generate(model, inp, 20, 20)\n",
    "print(time.time()-start)\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5601e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(out)):\n",
    "    assert out[0,i]==oracle[0,i], i\n",
    "print(\"Exact match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbox_oracle(inp):\n",
    "    out = oracle[:,inp.size(1):inp.size(1)+4].clone()\n",
    "    nzero = torch.randint(5,(1,))\n",
    "    if nzero > 0:\n",
    "        out[:,-nzero:] = 0\n",
    "    return out, min(oracle.size(1), inp.size(1)+4) - inp.size(1)\n",
    "\n",
    "def speculative_generate(\n",
    "    model: Union[Callable, torch.nn.Module],\n",
    "    input_ids: torch.LongTensor,\n",
    "    max_seq_len: int = 2048,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 10,\n",
    "    num_beams: int = 1,\n",
    "    smallmodel: Callable = blackbox_oracle,\n",
    "):\n",
    "    do_sample = False\n",
    "    use_cache = True\n",
    "    batched = False\n",
    "    if num_beams != 1:\n",
    "        raise NotImplementedError(\"generate() does yet not support beam search\")\n",
    "    if type(input_ids) == torch.Tensor:\n",
    "        if input_ids.dim() != 1:\n",
    "            batched = True\n",
    "    else:\n",
    "        raise RuntimeError(\"generate() requires a tensor of token ids as the prefix\")\n",
    "\n",
    "    if not batched:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    result = input_ids\n",
    "    next_input = input_ids\n",
    "    kwargs = dict()\n",
    "    kwargs[\"past_key_value_states\"] = None\n",
    "    kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "    n_gen = 0\n",
    "    n_steps = 0\n",
    "    while n_gen < max_new_tokens:\n",
    "        n_steps += 1\n",
    "        input_ids = next_input[:, -max_seq_len:]\n",
    "        adds, n_adds = smallmodel(result)\n",
    "        input_ids = torch.cat([input_ids, adds], dim=-1)\n",
    "        print(\"Speculation:\", input_ids)\n",
    "        output = model.forward(input_ids, **kwargs)\n",
    "        if use_cache:\n",
    "            logits, past_key_value_states = output\n",
    "        else:\n",
    "            logits = output\n",
    "        logits = logits[:, -n_adds-1:, :]\n",
    "\n",
    "        if do_sample:\n",
    "            # get logits from last value in sequence and scale\n",
    "#             logits = logits / temperature\n",
    "#             if top_k:\n",
    "#                 v, _ = torch.topk(logits, top_k)\n",
    "#                 logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "#             probs = F.softmax(logits, dim=-1)\n",
    "#             next_val = torch.multinomial(probs, num_samples=1)\n",
    "            pass\n",
    "        else:\n",
    "            next_vals = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Check correctness of smallmodel predictions\n",
    "        n_correct = 0\n",
    "        while n_correct < n_adds and next_vals[0,n_correct] == input_ids[0,-n_adds+n_correct]:\n",
    "            n_correct += 1\n",
    "        print(\"Verification:\", next_vals, n_correct)\n",
    "        \n",
    "        # Toss any wrong smallmodel outputs\n",
    "        next_vals = next_vals[:,:n_correct+1]\n",
    "        n_gen += n_correct+1\n",
    "            \n",
    "        n_wrong = n_adds - n_correct\n",
    "        if use_cache:\n",
    "            # kv updates are required for torch.compile with\n",
    "            # mode='reduce-overhead'\n",
    "            n_kv_s = []\n",
    "            for layer_idx in range(len(past_key_value_states)):\n",
    "                n_kv_s.append([])\n",
    "                for tensor_idx in range(len(past_key_value_states[layer_idx])):\n",
    "                    base = past_key_value_states[layer_idx][tensor_idx]\n",
    "                    if n_wrong > 0:\n",
    "                        base = base[:,:,:-n_wrong]\n",
    "                    n_kv_s[layer_idx].append(\n",
    "                        base.clone(memory_format=torch.contiguous_format).detach()\n",
    "                    )\n",
    "                    # torch._dynamo.mark_dynamic(n_kv_s[layer_idx][tensor_idx], 2)\n",
    "            kwargs[\"past_key_value_states\"] = n_kv_s\n",
    "\n",
    "        result = torch.cat((result, next_vals), dim=-1)\n",
    "        print(\"Updated output:\", result)\n",
    "        print()\n",
    "\n",
    "        if use_cache:\n",
    "            next_input = next_vals[:,-1].unsqueeze(-1)\n",
    "        else:\n",
    "            next_input = result\n",
    "\n",
    "    if not batched:\n",
    "        result = result[0]\n",
    "    return result, n_steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
